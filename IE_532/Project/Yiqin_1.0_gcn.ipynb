{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#Reference\" data-toc-modified-id=\"Reference-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Reference</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#Code\" data-toc-modified-id=\"Code-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Code</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#generate-graph\" data-toc-modified-id=\"generate-graph-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>generate graph</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#simple-neural-network\" data-toc-modified-id=\"simple-neural-network-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>simple neural network</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#测试前要看一下\" data-toc-modified-id=\"测试前要看一下-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>测试前要看一下</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@bgoncalves/network-effects-explained-pagerank-and-preferential-attachment-61fdf93d023a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"web-Stanford.txt\",\"r\") as fil:\n",
    "    for _ in range(4):\n",
    "        next(fil)\n",
    "    data=[tuple([int(j) for j in i.strip().split()]) for i in fil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from(data)\n",
    "\n",
    "pr = nx.pagerank(G, alpha=0.9)   #作为true response?\n",
    "\n",
    "for i in pr.keys():\n",
    "    G.nodes[i]['score']=pr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.561215144991757e-07"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes[1]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=len(G.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gcn.utils import *\n",
    "from gcn.models import Model,MLP\n",
    "from gcn.layers import *\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试前要看一下\n",
    "+ 之前学习率填了0.05，明显训练的时候有了误差来回变大减小又变大再减小的情况，应该要调小一点，之前作业里是0.01,我一时兴起又改大了……\n",
    "+ 用networkx里的pagerank算法出来的分数是在0-1之间，所以最后的误差出来即使是0.几，也不确定是不是很好，这里是100个点做validation。\n",
    "+ epoch可能也要调，学习率0.05的时候300差不多收敛了，但是不知道学习率改了之后是什么情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'  #这行应该并没有什么用\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.05, 'Initial learning rate.')\n",
    "#flags.DEFINE_float('learning_rate', 0.1, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 300, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "#flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "#flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 100, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "#--------------- Your code here --------------#\n",
    "\n",
    "def masked_mean_square_error(preds,labels,mask):\n",
    "    \"\"\"L2 loss with masking.\"\"\"\n",
    "    loss = tf.nn.l2_loss(preds - labels)   #这里是可以考虑ReLU什么的么，还没细想，只是突然想到的可能\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "#--------------------------------------------#\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_mean_square_error(self.outputs, self.placeholders['labels'],\n",
    "                                                 self.placeholders['labels_mask'])\n",
    "#        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "#                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=FLAGS.hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return self.outputs    #tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    #adj_square = normalize_adj(np.power(adj, 2).tocoo())   #A^2 有空可以尝试\n",
    "    return sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly generate graph with 500 nodes\n",
    "adj = nx.adjacency_matrix(G)\n",
    "\n",
    "# Generate feature matrix  #there is difference between different version of networkx, 这里是用networkx 2.0. 原来1.1的用法注释掉了\n",
    "features = sp.vstack(list(j[1] for j in G.in_degree(G.nodes())),dtype = 'float32').tolil()\n",
    "#features = sp.vstack(list(G.in_degree(G.nodes()).values()),dtype = 'float32').tolil()   \n",
    "\n",
    "# Feed labels to 20 nodes\n",
    "label = []\n",
    "for i in G.nodes():\n",
    "    label.append(G.nodes[i]['score'])\n",
    "        \n",
    "alabel = np.array(label, dtype='float64')\n",
    "\n",
    "idx_train = range(20000)\n",
    "idx_val = range(20000, 20000 + 100)\n",
    "\n",
    "y_train = np.zeros([n, 1])   #n is number of total nodes, =281903\n",
    "train_mask = np.zeros([n, ], dtype = 'bool')\n",
    "for i in idx_train:\n",
    "    y_train[i] = alabel[i]\n",
    "    train_mask[i] = True\n",
    "\n",
    "y_val = np.zeros([n, 1])\n",
    "val_mask = np.zeros([n, ], dtype = 'bool')\n",
    "for i in idx_val:\n",
    "    y_val[i] = alabel[i]\n",
    "    val_mask[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\py36\\lib\\site-packages\\gcn-1.0-py3.6.egg\\gcn\\utils.py:97: RuntimeWarning: divide by zero encountered in power\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "#adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 15932818.00000 train_acc= 0.99995 val_loss= 6530353.00000 val_acc= 1.00000 time= 4.45378\n",
      "Epoch: 0002 train_loss= 9549798.00000 train_acc= 0.99995 val_loss= 1582841.00000 val_acc= 1.00000 time= 4.42326\n",
      "Epoch: 0003 train_loss= 2399162.00000 train_acc= 0.99995 val_loss= 5216.68408 val_acc= 1.00000 time= 4.45093\n",
      "Epoch: 0004 train_loss= 1005767.43750 train_acc= 0.99995 val_loss= 798486.62500 val_acc= 1.00000 time= 4.44427\n",
      "Epoch: 0005 train_loss= 1645399.37500 train_acc= 0.99995 val_loss= 2148284.25000 val_acc= 1.00000 time= 4.41487\n",
      "Epoch: 0006 train_loss= 3304748.00000 train_acc= 0.99995 val_loss= 2800197.00000 val_acc= 1.00000 time= 4.40683\n",
      "Epoch: 0007 train_loss= 4370506.00000 train_acc= 0.99995 val_loss= 2251100.00000 val_acc= 1.00000 time= 4.39124\n",
      "Epoch: 0008 train_loss= 2369627.75000 train_acc= 0.99995 val_loss= 1616308.75000 val_acc= 1.00000 time= 4.40668\n",
      "Epoch: 0009 train_loss= 1896579.75000 train_acc= 0.99995 val_loss= 668010.50000 val_acc= 1.00000 time= 4.38774\n",
      "Epoch: 0010 train_loss= 1445823.37500 train_acc= 0.99995 val_loss= 157345.25000 val_acc= 1.00000 time= 4.35996\n",
      "Epoch: 0011 train_loss= 526851.43750 train_acc= 0.99995 val_loss= 17319.90430 val_acc= 1.00000 time= 4.35984\n",
      "Epoch: 0012 train_loss= 807276.75000 train_acc= 0.99995 val_loss= 176934.56250 val_acc= 1.00000 time= 4.36012\n",
      "Epoch: 0013 train_loss= 721649.00000 train_acc= 0.99995 val_loss= 309158.96875 val_acc= 1.00000 time= 4.35984\n",
      "Epoch: 0014 train_loss= 269963.96875 train_acc= 0.99995 val_loss= 423172.09375 val_acc= 1.00000 time= 4.37560\n",
      "Epoch: 0015 train_loss= 550583.18750 train_acc= 0.99995 val_loss= 456861.93750 val_acc= 1.00000 time= 4.35984\n",
      "Epoch: 0016 train_loss= 504468.68750 train_acc= 0.99995 val_loss= 456288.50000 val_acc= 1.00000 time= 4.39648\n",
      "Epoch: 0017 train_loss= 890447.12500 train_acc= 0.99995 val_loss= 368860.62500 val_acc= 1.00000 time= 4.37552\n",
      "Epoch: 0018 train_loss= 841873.87500 train_acc= 0.99995 val_loss= 242044.37500 val_acc= 1.00000 time= 4.37571\n",
      "Epoch: 0019 train_loss= 483697.59375 train_acc= 0.99995 val_loss= 129090.87500 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0020 train_loss= 164014.78125 train_acc= 0.99995 val_loss= 53289.74609 val_acc= 1.00000 time= 4.37549\n",
      "Epoch: 0021 train_loss= 187646.76562 train_acc= 0.99995 val_loss= 12967.51758 val_acc= 1.00000 time= 4.37543\n",
      "Epoch: 0022 train_loss= 152265.25000 train_acc= 0.99995 val_loss= 520.12671 val_acc= 1.00000 time= 4.35984\n",
      "Epoch: 0023 train_loss= 135377.89062 train_acc= 0.99995 val_loss= 3461.77588 val_acc= 1.00000 time= 4.37549\n",
      "Epoch: 0024 train_loss= 71353.84375 train_acc= 0.99995 val_loss= 14288.74414 val_acc= 1.00000 time= 4.35448\n",
      "Epoch: 0025 train_loss= 235455.43750 train_acc= 0.99995 val_loss= 26457.03125 val_acc= 1.00000 time= 4.40672\n",
      "Epoch: 0026 train_loss= 67005.57031 train_acc= 0.99995 val_loss= 37912.35938 val_acc= 1.00000 time= 4.35551\n",
      "Epoch: 0027 train_loss= 121431.12500 train_acc= 0.99995 val_loss= 45175.38281 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0028 train_loss= 193628.07812 train_acc= 0.99995 val_loss= 48505.25781 val_acc= 1.00000 time= 4.37754\n",
      "Epoch: 0029 train_loss= 95924.44531 train_acc= 0.99995 val_loss= 49155.28906 val_acc= 1.00000 time= 4.35996\n",
      "Epoch: 0030 train_loss= 218818.79688 train_acc= 0.99995 val_loss= 44445.63672 val_acc= 1.00000 time= 4.35984\n",
      "Epoch: 0031 train_loss= 196952.79688 train_acc= 0.99995 val_loss= 35798.65234 val_acc= 1.00000 time= 4.38491\n",
      "Epoch: 0032 train_loss= 85422.99219 train_acc= 0.99995 val_loss= 26615.84961 val_acc= 1.00000 time= 4.37551\n",
      "Epoch: 0033 train_loss= 135589.32812 train_acc= 0.99995 val_loss= 17570.96289 val_acc= 1.00000 time= 4.36014\n",
      "Epoch: 0034 train_loss= 111155.49219 train_acc= 0.99995 val_loss= 9699.96387 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0035 train_loss= 70013.03125 train_acc= 0.99995 val_loss= 4099.29785 val_acc= 1.00000 time= 4.37557\n",
      "Epoch: 0036 train_loss= 32117.49219 train_acc= 0.99995 val_loss= 1048.17517 val_acc= 1.00000 time= 4.35983\n",
      "Epoch: 0037 train_loss= 59354.24609 train_acc= 0.99995 val_loss= 7.54496 val_acc= 1.00000 time= 4.39109\n",
      "Epoch: 0038 train_loss= 20424.31445 train_acc= 0.99995 val_loss= 595.21771 val_acc= 1.00000 time= 4.39460\n",
      "Epoch: 0039 train_loss= 29439.14062 train_acc= 0.99995 val_loss= 2361.36914 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0040 train_loss= 54124.60156 train_acc= 0.99995 val_loss= 4661.53320 val_acc= 1.00000 time= 4.35987\n",
      "Epoch: 0041 train_loss= 37632.24219 train_acc= 0.99995 val_loss= 7262.03564 val_acc= 1.00000 time= 4.40669\n",
      "Epoch: 0042 train_loss= 61715.48438 train_acc= 0.99995 val_loss= 9637.51855 val_acc= 1.00000 time= 4.40672\n",
      "Epoch: 0043 train_loss= 35073.23047 train_acc= 0.99995 val_loss= 11534.49609 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0044 train_loss= 46772.38672 train_acc= 0.99995 val_loss= 12595.07715 val_acc= 1.00000 time= 4.36323\n",
      "Epoch: 0045 train_loss= 41804.28125 train_acc= 0.99995 val_loss= 13087.35059 val_acc= 1.00000 time= 4.38435\n",
      "Epoch: 0046 train_loss= 34734.23047 train_acc= 0.99995 val_loss= 12831.17285 val_acc= 1.00000 time= 4.40672\n",
      "Epoch: 0047 train_loss= 55962.90234 train_acc= 0.99995 val_loss= 12003.53809 val_acc= 1.00000 time= 4.35996\n",
      "Epoch: 0048 train_loss= 16627.50195 train_acc= 0.99995 val_loss= 11245.95215 val_acc= 1.00000 time= 4.41782\n",
      "Epoch: 0049 train_loss= 26372.25000 train_acc= 0.99995 val_loss= 10018.68164 val_acc= 1.00000 time= 4.35988\n",
      "Epoch: 0050 train_loss= 18304.72656 train_acc= 0.99995 val_loss= 8685.66797 val_acc= 1.00000 time= 4.36425\n",
      "Epoch: 0051 train_loss= 14016.74219 train_acc= 0.99995 val_loss= 7353.17822 val_acc= 1.00000 time= 4.39119\n",
      "Epoch: 0052 train_loss= 51167.51953 train_acc= 0.99995 val_loss= 5764.28369 val_acc= 1.00000 time= 4.40747\n",
      "Epoch: 0053 train_loss= 14365.38281 train_acc= 0.99995 val_loss= 4470.83252 val_acc= 1.00000 time= 4.35989\n",
      "Epoch: 0054 train_loss= 39902.49609 train_acc= 0.99995 val_loss= 3221.69141 val_acc= 1.00000 time= 4.35984\n",
      "Epoch: 0055 train_loss= 12718.15332 train_acc= 0.99995 val_loss= 2157.20435 val_acc= 1.00000 time= 4.40785\n",
      "Epoch: 0056 train_loss= 26010.37305 train_acc= 0.99995 val_loss= 1346.97327 val_acc= 1.00000 time= 4.38952\n",
      "Epoch: 0057 train_loss= 17216.88477 train_acc= 0.99995 val_loss= 720.13013 val_acc= 1.00000 time= 4.37543\n",
      "Epoch: 0058 train_loss= 16912.19531 train_acc= 0.99995 val_loss= 291.64969 val_acc= 1.00000 time= 4.39514\n",
      "Epoch: 0059 train_loss= 19848.54883 train_acc= 0.99995 val_loss= 86.59690 val_acc= 1.00000 time= 4.36799\n",
      "Epoch: 0060 train_loss= 10334.41406 train_acc= 0.99995 val_loss= 6.34415 val_acc= 1.00000 time= 4.36586\n",
      "Epoch: 0061 train_loss= 16683.02148 train_acc= 0.99995 val_loss= 21.66101 val_acc= 1.00000 time= 4.35984\n",
      "Epoch: 0062 train_loss= 14611.77734 train_acc= 0.99995 val_loss= 133.26250 val_acc= 1.00000 time= 4.35983\n",
      "Epoch: 0063 train_loss= 14007.88867 train_acc= 0.99995 val_loss= 314.93820 val_acc= 1.00000 time= 4.39109\n",
      "Epoch: 0064 train_loss= 15351.91406 train_acc= 0.99995 val_loss= 579.28326 val_acc= 1.00000 time= 4.42234\n",
      "Epoch: 0065 train_loss= 10800.80664 train_acc= 0.99995 val_loss= 818.67889 val_acc= 1.00000 time= 4.35991\n",
      "Epoch: 0066 train_loss= 19031.11523 train_acc= 0.99995 val_loss= 1055.34009 val_acc= 1.00000 time= 4.36786\n",
      "Epoch: 0067 train_loss= 8159.92432 train_acc= 0.99995 val_loss= 1306.38123 val_acc= 1.00000 time= 4.37556\n",
      "Epoch: 0068 train_loss= 10887.09180 train_acc= 0.99995 val_loss= 1487.91687 val_acc= 1.00000 time= 4.37649\n",
      "Epoch: 0069 train_loss= 7650.40723 train_acc= 0.99995 val_loss= 1682.09436 val_acc= 1.00000 time= 4.39112\n",
      "Epoch: 0070 train_loss= 10968.73926 train_acc= 0.99995 val_loss= 1798.46582 val_acc= 1.00000 time= 4.35980\n",
      "Epoch: 0071 train_loss= 12458.21094 train_acc= 0.99995 val_loss= 1757.55005 val_acc= 1.00000 time= 4.36056\n",
      "Epoch: 0072 train_loss= 10521.75000 train_acc= 0.99995 val_loss= 1659.29126 val_acc= 1.00000 time= 4.37710\n",
      "Epoch: 0073 train_loss= 16574.11523 train_acc= 0.99995 val_loss= 1446.24414 val_acc= 1.00000 time= 4.37581\n",
      "Epoch: 0074 train_loss= 16891.16797 train_acc= 0.99995 val_loss= 1174.62915 val_acc= 1.00000 time= 4.36067\n",
      "Epoch: 0075 train_loss= 18328.57031 train_acc= 0.99995 val_loss= 874.01013 val_acc= 1.00000 time= 4.37597\n",
      "Epoch: 0076 train_loss= 9027.10059 train_acc= 0.99995 val_loss= 597.50122 val_acc= 1.00000 time= 4.36052\n",
      "Epoch: 0077 train_loss= 9919.80859 train_acc= 0.99995 val_loss= 376.03021 val_acc= 1.00000 time= 4.40711\n",
      "Epoch: 0078 train_loss= 10835.31445 train_acc= 0.99995 val_loss= 197.61980 val_acc= 1.00000 time= 4.37587\n",
      "Epoch: 0079 train_loss= 11110.38281 train_acc= 0.99995 val_loss= 82.59387 val_acc= 1.00000 time= 4.36081\n",
      "Epoch: 0080 train_loss= 8877.29883 train_acc= 0.99995 val_loss= 25.11000 val_acc= 1.00000 time= 4.37645\n",
      "Epoch: 0081 train_loss= 3786.61621 train_acc= 0.99995 val_loss= 2.10753 val_acc= 1.00000 time= 4.36255\n",
      "Epoch: 0082 train_loss= 5557.60840 train_acc= 0.99995 val_loss= 4.75918 val_acc= 1.00000 time= 4.43831\n",
      "Epoch: 0083 train_loss= 5446.57373 train_acc= 0.99995 val_loss= 35.27009 val_acc= 1.00000 time= 4.36013\n",
      "Epoch: 0084 train_loss= 9753.23145 train_acc= 0.99995 val_loss= 100.08374 val_acc= 1.00000 time= 4.37605\n",
      "Epoch: 0085 train_loss= 11709.77441 train_acc= 0.99995 val_loss= 192.66751 val_acc= 1.00000 time= 4.39109\n",
      "Epoch: 0086 train_loss= 8216.32520 train_acc= 0.99995 val_loss= 323.41824 val_acc= 1.00000 time= 4.37939\n",
      "Epoch: 0087 train_loss= 4664.50781 train_acc= 0.99995 val_loss= 474.90833 val_acc= 1.00000 time= 4.40683\n",
      "Epoch: 0088 train_loss= 4694.24561 train_acc= 0.99995 val_loss= 617.21906 val_acc= 1.00000 time= 4.40358\n",
      "Epoch: 0089 train_loss= 11970.09668 train_acc= 0.99995 val_loss= 706.55939 val_acc= 1.00000 time= 4.39155\n",
      "Epoch: 0090 train_loss= 3652.04858 train_acc= 0.99995 val_loss= 791.49786 val_acc= 1.00000 time= 4.40711\n",
      "Epoch: 0091 train_loss= 5282.28662 train_acc= 0.99995 val_loss= 832.96790 val_acc= 1.00000 time= 4.42272\n",
      "Epoch: 0092 train_loss= 5961.58643 train_acc= 0.99995 val_loss= 827.29755 val_acc= 1.00000 time= 4.42305\n",
      "Epoch: 0093 train_loss= 8820.85352 train_acc= 0.99995 val_loss= 789.53400 val_acc= 1.00000 time= 4.35277\n",
      "Epoch: 0094 train_loss= 2465.19360 train_acc= 0.99995 val_loss= 767.08228 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0095 train_loss= 7739.98730 train_acc= 0.99995 val_loss= 701.87976 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0096 train_loss= 6045.81152 train_acc= 0.99995 val_loss= 659.57153 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0097 train_loss= 4917.44678 train_acc= 0.99995 val_loss= 599.60638 val_acc= 1.00000 time= 4.38151\n",
      "Epoch: 0098 train_loss= 5055.96387 train_acc= 0.99995 val_loss= 516.59760 val_acc= 1.00000 time= 4.40672\n",
      "Epoch: 0099 train_loss= 4128.22461 train_acc= 0.99995 val_loss= 452.08118 val_acc= 1.00000 time= 4.40672\n",
      "Epoch: 0100 train_loss= 4189.69092 train_acc= 0.99995 val_loss= 396.11584 val_acc= 1.00000 time= 4.37555\n",
      "Epoch: 0101 train_loss= 5269.32031 train_acc= 0.99995 val_loss= 321.94473 val_acc= 1.00000 time= 4.39109\n",
      "Epoch: 0102 train_loss= 2946.53174 train_acc= 0.99995 val_loss= 264.71213 val_acc= 1.00000 time= 4.36005\n",
      "Epoch: 0103 train_loss= 5438.85352 train_acc= 0.99995 val_loss= 198.91211 val_acc= 1.00000 time= 4.40675\n",
      "Epoch: 0104 train_loss= 6252.35449 train_acc= 0.99995 val_loss= 131.83345 val_acc= 1.00000 time= 4.37547\n",
      "Epoch: 0105 train_loss= 5096.11230 train_acc= 0.99995 val_loss= 85.68369 val_acc= 1.00000 time= 4.36047\n",
      "Epoch: 0106 train_loss= 3961.63037 train_acc= 0.99995 val_loss= 58.69425 val_acc= 1.00000 time= 4.37081\n",
      "Epoch: 0107 train_loss= 8626.58008 train_acc= 0.99995 val_loss= 35.34989 val_acc= 1.00000 time= 4.37809\n",
      "Epoch: 0108 train_loss= 2233.93188 train_acc= 0.99995 val_loss= 22.04230 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0109 train_loss= 5267.11133 train_acc= 0.99995 val_loss= 13.18678 val_acc= 1.00000 time= 4.39109\n",
      "Epoch: 0110 train_loss= 2284.26123 train_acc= 0.99995 val_loss= 9.86901 val_acc= 1.00000 time= 4.37555\n",
      "Epoch: 0111 train_loss= 6751.67529 train_acc= 0.99995 val_loss= 6.23961 val_acc= 1.00000 time= 4.35992\n",
      "Epoch: 0112 train_loss= 2201.69287 train_acc= 0.99995 val_loss= 3.68069 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0113 train_loss= 2821.94116 train_acc= 0.99995 val_loss= 1.16203 val_acc= 1.00000 time= 4.37621\n",
      "Epoch: 0114 train_loss= 3929.91431 train_acc= 0.99995 val_loss= 0.05467 val_acc= 1.00000 time= 4.36876\n",
      "Epoch: 0115 train_loss= 2740.22876 train_acc= 0.99995 val_loss= 0.00897 val_acc= 1.00000 time= 4.36009\n",
      "Epoch: 0116 train_loss= 3371.67334 train_acc= 0.99995 val_loss= 0.46656 val_acc= 1.00000 time= 4.35990\n",
      "Epoch: 0117 train_loss= 3617.62744 train_acc= 0.99995 val_loss= 1.51532 val_acc= 1.00000 time= 4.35991\n",
      "Epoch: 0118 train_loss= 3757.10522 train_acc= 0.99995 val_loss= 2.23443 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0119 train_loss= 1846.45312 train_acc= 0.99995 val_loss= 2.32961 val_acc= 1.00000 time= 4.43797\n",
      "Epoch: 0120 train_loss= 2131.79712 train_acc= 0.99995 val_loss= 2.11575 val_acc= 1.00000 time= 4.42289\n",
      "Epoch: 0121 train_loss= 1636.88550 train_acc= 0.99995 val_loss= 1.89362 val_acc= 1.00000 time= 4.42234\n",
      "Epoch: 0122 train_loss= 1601.15186 train_acc= 0.99995 val_loss= 1.10491 val_acc= 1.00000 time= 4.39109\n",
      "Epoch: 0123 train_loss= 2524.56177 train_acc= 0.99995 val_loss= 0.25094 val_acc= 1.00000 time= 4.45135\n",
      "Epoch: 0124 train_loss= 3826.48975 train_acc= 0.99995 val_loss= 0.03097 val_acc= 1.00000 time= 4.59555\n",
      "Epoch: 0125 train_loss= 2348.17651 train_acc= 0.99995 val_loss= 0.11430 val_acc= 1.00000 time= 4.56396\n",
      "Epoch: 0126 train_loss= 2804.56982 train_acc= 0.99995 val_loss= 0.49985 val_acc= 1.00000 time= 4.61229\n",
      "Epoch: 0127 train_loss= 7405.20264 train_acc= 0.99995 val_loss= 3.34196 val_acc= 1.00000 time= 4.47077\n",
      "Epoch: 0128 train_loss= 4229.33594 train_acc= 0.99995 val_loss= 11.33964 val_acc= 1.00000 time= 4.39134\n",
      "Epoch: 0129 train_loss= 1265.51270 train_acc= 0.99995 val_loss= 22.19395 val_acc= 1.00000 time= 4.39584\n",
      "Epoch: 0130 train_loss= 2006.83008 train_acc= 0.99995 val_loss= 34.29877 val_acc= 1.00000 time= 4.40671\n",
      "Epoch: 0131 train_loss= 4345.52295 train_acc= 0.99995 val_loss= 50.42920 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0132 train_loss= 2061.38770 train_acc= 0.99995 val_loss= 72.14294 val_acc= 1.00000 time= 4.35983\n",
      "Epoch: 0133 train_loss= 1814.97339 train_acc= 0.99995 val_loss= 88.12713 val_acc= 1.00000 time= 4.39753\n",
      "Epoch: 0134 train_loss= 1536.97375 train_acc= 0.99995 val_loss= 103.84067 val_acc= 1.00000 time= 4.49116\n",
      "Epoch: 0135 train_loss= 4591.27686 train_acc= 0.99995 val_loss= 117.31784 val_acc= 1.00000 time= 4.42275\n",
      "Epoch: 0136 train_loss= 4734.44238 train_acc= 0.99995 val_loss= 124.03947 val_acc= 1.00000 time= 4.48519\n",
      "Epoch: 0137 train_loss= 1484.05981 train_acc= 0.99995 val_loss= 123.25190 val_acc= 1.00000 time= 4.45389\n",
      "Epoch: 0138 train_loss= 4022.69141 train_acc= 0.99995 val_loss= 116.89553 val_acc= 1.00000 time= 4.47020\n",
      "Epoch: 0139 train_loss= 1464.91040 train_acc= 0.99995 val_loss= 115.52480 val_acc= 1.00000 time= 4.42245\n",
      "Epoch: 0140 train_loss= 2326.01587 train_acc= 0.99995 val_loss= 108.53323 val_acc= 1.00000 time= 4.43847\n",
      "Epoch: 0141 train_loss= 1874.68762 train_acc= 0.99995 val_loss= 105.22340 val_acc= 1.00000 time= 4.46550\n",
      "Epoch: 0142 train_loss= 2565.31738 train_acc= 0.99995 val_loss= 102.57816 val_acc= 1.00000 time= 4.59028\n",
      "Epoch: 0143 train_loss= 2169.98950 train_acc= 0.99995 val_loss= 95.89807 val_acc= 1.00000 time= 4.45923\n",
      "Epoch: 0144 train_loss= 891.65820 train_acc= 0.99995 val_loss= 91.12190 val_acc= 1.00000 time= 4.44640\n",
      "Epoch: 0145 train_loss= 2311.24438 train_acc= 0.99995 val_loss= 83.62850 val_acc= 1.00000 time= 4.45359\n",
      "Epoch: 0146 train_loss= 1268.92615 train_acc= 0.99995 val_loss= 79.63829 val_acc= 1.00000 time= 4.42234\n",
      "Epoch: 0147 train_loss= 1285.50854 train_acc= 0.99995 val_loss= 80.73961 val_acc= 1.00000 time= 4.43859\n",
      "Epoch: 0148 train_loss= 2184.02173 train_acc= 0.99995 val_loss= 87.71632 val_acc= 1.00000 time= 4.43797\n",
      "Epoch: 0149 train_loss= 1459.21399 train_acc= 0.99995 val_loss= 91.21114 val_acc= 1.00000 time= 4.43797\n",
      "Epoch: 0150 train_loss= 1672.18628 train_acc= 0.99995 val_loss= 94.90730 val_acc= 1.00000 time= 4.44585\n",
      "Epoch: 0151 train_loss= 2890.85596 train_acc= 0.99995 val_loss= 106.69978 val_acc= 1.00000 time= 4.42268\n",
      "Epoch: 0152 train_loss= 1478.90796 train_acc= 0.99995 val_loss= 114.15267 val_acc= 1.00000 time= 4.44881\n",
      "Epoch: 0153 train_loss= 1143.14722 train_acc= 0.99995 val_loss= 117.61507 val_acc= 1.00000 time= 4.48485\n",
      "Epoch: 0154 train_loss= 873.49170 train_acc= 0.99995 val_loss= 119.67734 val_acc= 1.00000 time= 4.45366\n",
      "Epoch: 0155 train_loss= 2606.18945 train_acc= 0.99995 val_loss= 117.20149 val_acc= 1.00000 time= 4.43802\n",
      "Epoch: 0156 train_loss= 990.39868 train_acc= 0.99995 val_loss= 115.85544 val_acc= 1.00000 time= 4.51621\n",
      "Epoch: 0157 train_loss= 2410.33276 train_acc= 0.99995 val_loss= 119.21577 val_acc= 1.00000 time= 4.57859\n",
      "Epoch: 0158 train_loss= 2232.15137 train_acc= 0.99995 val_loss= 114.03809 val_acc= 1.00000 time= 4.51889\n",
      "Epoch: 0159 train_loss= 829.24091 train_acc= 0.99995 val_loss= 103.59534 val_acc= 1.00000 time= 4.42234\n",
      "Epoch: 0160 train_loss= 2399.60474 train_acc= 0.99995 val_loss= 89.27169 val_acc= 1.00000 time= 4.43805\n",
      "Epoch: 0161 train_loss= 3293.87036 train_acc= 0.99995 val_loss= 77.80622 val_acc= 1.00000 time= 4.45368\n",
      "Epoch: 0162 train_loss= 1757.20996 train_acc= 0.99995 val_loss= 65.55698 val_acc= 1.00000 time= 4.40678\n",
      "Epoch: 0163 train_loss= 1552.30457 train_acc= 0.99995 val_loss= 53.81926 val_acc= 1.00000 time= 4.42234\n",
      "Epoch: 0164 train_loss= 944.09741 train_acc= 0.99995 val_loss= 45.36694 val_acc= 1.00000 time= 4.45442\n",
      "Epoch: 0165 train_loss= 1645.96655 train_acc= 0.99995 val_loss= 42.70015 val_acc= 1.00000 time= 4.48535\n",
      "Epoch: 0166 train_loss= 683.73248 train_acc= 0.99995 val_loss= 38.88081 val_acc= 1.00000 time= 4.45432\n",
      "Epoch: 0167 train_loss= 731.33423 train_acc= 0.99995 val_loss= 34.28728 val_acc= 1.00000 time= 4.44102\n",
      "Epoch: 0168 train_loss= 1095.41138 train_acc= 0.99995 val_loss= 30.70731 val_acc= 1.00000 time= 4.46665\n",
      "Epoch: 0169 train_loss= 998.37384 train_acc= 0.99995 val_loss= 27.17597 val_acc= 1.00000 time= 4.39109\n",
      "Epoch: 0170 train_loss= 2027.46606 train_acc= 0.99995 val_loss= 20.90094 val_acc= 1.00000 time= 4.39667\n",
      "Epoch: 0171 train_loss= 772.55054 train_acc= 0.99995 val_loss= 14.80871 val_acc= 1.00000 time= 4.40672\n",
      "Epoch: 0172 train_loss= 1130.82861 train_acc= 0.99995 val_loss= 9.04509 val_acc= 1.00000 time= 4.42234\n",
      "Epoch: 0173 train_loss= 1102.46533 train_acc= 0.99995 val_loss= 6.25771 val_acc= 1.00000 time= 4.43805\n",
      "Epoch: 0174 train_loss= 1344.57617 train_acc= 0.99995 val_loss= 5.42900 val_acc= 1.00000 time= 4.48585\n",
      "Epoch: 0175 train_loss= 1117.36548 train_acc= 0.99995 val_loss= 4.51300 val_acc= 1.00000 time= 4.43797\n",
      "Epoch: 0176 train_loss= 789.00555 train_acc= 0.99995 val_loss= 4.47507 val_acc= 1.00000 time= 4.44020\n",
      "Epoch: 0177 train_loss= 1061.11536 train_acc= 0.99995 val_loss= 4.30008 val_acc= 1.00000 time= 4.43769\n",
      "Epoch: 0178 train_loss= 699.45874 train_acc= 0.99995 val_loss= 4.44331 val_acc= 1.00000 time= 4.43797\n",
      "Epoch: 0179 train_loss= 568.82471 train_acc= 0.99995 val_loss= 4.26590 val_acc= 1.00000 time= 4.42340\n",
      "Epoch: 0180 train_loss= 1104.54248 train_acc= 0.99995 val_loss= 3.86292 val_acc= 1.00000 time= 4.61010\n",
      "Epoch: 0181 train_loss= 1557.24390 train_acc= 0.99995 val_loss= 4.19203 val_acc= 1.00000 time= 4.48519\n",
      "Epoch: 0182 train_loss= 1810.01978 train_acc= 0.99995 val_loss= 3.32974 val_acc= 1.00000 time= 4.54743\n",
      "Epoch: 0183 train_loss= 1461.53967 train_acc= 0.99995 val_loss= 1.81844 val_acc= 1.00000 time= 4.45360\n",
      "Epoch: 0184 train_loss= 1269.03918 train_acc= 0.99995 val_loss= 0.56595 val_acc= 1.00000 time= 4.45360\n",
      "Epoch: 0185 train_loss= 1185.13464 train_acc= 0.99995 val_loss= 0.11260 val_acc= 1.00000 time= 4.45371\n",
      "Epoch: 0186 train_loss= 954.65918 train_acc= 0.99995 val_loss= 0.05242 val_acc= 1.00000 time= 4.43797\n",
      "Epoch: 0187 train_loss= 1510.34314 train_acc= 0.99995 val_loss= 0.01751 val_acc= 1.00000 time= 4.42996\n",
      "Epoch: 0188 train_loss= 837.51202 train_acc= 0.99995 val_loss= 0.10158 val_acc= 1.00000 time= 4.46922\n",
      "Epoch: 0189 train_loss= 569.28094 train_acc= 0.99995 val_loss= 0.19245 val_acc= 1.00000 time= 4.48485\n",
      "Epoch: 0190 train_loss= 1751.10938 train_acc= 0.99995 val_loss= 0.03437 val_acc= 1.00000 time= 4.46931\n",
      "Epoch: 0191 train_loss= 365.66003 train_acc= 0.99995 val_loss= 0.00023 val_acc= 1.00000 time= 4.42508\n",
      "Epoch: 0192 train_loss= 522.34467 train_acc= 0.99995 val_loss= 0.02335 val_acc= 1.00000 time= 4.47393\n",
      "Epoch: 0193 train_loss= 891.57886 train_acc= 0.99995 val_loss= 0.07777 val_acc= 1.00000 time= 4.45364\n",
      "Epoch: 0194 train_loss= 577.81091 train_acc= 0.99995 val_loss= 0.23013 val_acc= 1.00000 time= 4.44799\n",
      "Epoch: 0195 train_loss= 828.37433 train_acc= 0.99995 val_loss= 0.24929 val_acc= 1.00000 time= 4.42241\n",
      "Epoch: 0196 train_loss= 2602.86035 train_acc= 0.99995 val_loss= 0.77467 val_acc= 1.00000 time= 4.46055\n",
      "Epoch: 0197 train_loss= 1513.67029 train_acc= 0.99995 val_loss= 0.45147 val_acc= 1.00000 time= 4.42238\n",
      "Epoch: 0198 train_loss= 690.75824 train_acc= 0.99995 val_loss= 0.18331 val_acc= 1.00000 time= 4.43797\n",
      "Epoch: 0199 train_loss= 862.63470 train_acc= 0.99995 val_loss= 0.00031 val_acc= 1.00000 time= 4.40672\n",
      "Epoch: 0200 train_loss= 550.85138 train_acc= 0.99995 val_loss= 0.17711 val_acc= 1.00000 time= 4.40677\n",
      "Epoch: 0201 train_loss= 1957.60107 train_acc= 0.99995 val_loss= 0.55561 val_acc= 1.00000 time= 4.40785\n",
      "Epoch: 0202 train_loss= 1014.26233 train_acc= 0.99995 val_loss= 0.76317 val_acc= 1.00000 time= 4.43175\n",
      "Epoch: 0203 train_loss= 679.38123 train_acc= 0.99995 val_loss= 0.88145 val_acc= 1.00000 time= 4.50058\n",
      "Epoch: 0204 train_loss= 815.11407 train_acc= 0.99995 val_loss= 0.81945 val_acc= 1.00000 time= 4.56371\n",
      "Epoch: 0205 train_loss= 897.88257 train_acc= 0.99995 val_loss= 0.59135 val_acc= 1.00000 time= 4.45685\n",
      "Epoch: 0206 train_loss= 721.45886 train_acc= 0.99995 val_loss= 0.20956 val_acc= 1.00000 time= 4.50056\n",
      "Epoch: 0207 train_loss= 579.03217 train_acc= 0.99995 val_loss= 0.10991 val_acc= 1.00000 time= 4.59377\n",
      "Epoch: 0208 train_loss= 480.19839 train_acc= 0.99995 val_loss= 0.08566 val_acc= 1.00000 time= 4.53115\n",
      "Epoch: 0209 train_loss= 411.20740 train_acc= 0.99995 val_loss= 0.07244 val_acc= 1.00000 time= 4.49210\n",
      "Epoch: 0210 train_loss= 892.48682 train_acc= 0.99995 val_loss= 0.17181 val_acc= 1.00000 time= 4.36930\n",
      "Epoch: 0211 train_loss= 889.59552 train_acc= 0.99995 val_loss= 0.71255 val_acc= 1.00000 time= 4.48943\n",
      "Epoch: 0212 train_loss= 967.41736 train_acc= 0.99995 val_loss= 1.76285 val_acc= 1.00000 time= 4.47005\n",
      "Epoch: 0213 train_loss= 767.64447 train_acc= 0.99995 val_loss= 3.08758 val_acc= 1.00000 time= 4.46921\n",
      "Epoch: 0214 train_loss= 432.45822 train_acc= 0.99995 val_loss= 3.97382 val_acc= 1.00000 time= 4.45399\n",
      "Epoch: 0215 train_loss= 487.15393 train_acc= 0.99995 val_loss= 5.59168 val_acc= 1.00000 time= 4.51610\n",
      "Epoch: 0216 train_loss= 282.85211 train_acc= 0.99995 val_loss= 8.24365 val_acc= 1.00000 time= 4.46228\n",
      "Epoch: 0217 train_loss= 528.12500 train_acc= 0.99995 val_loss= 12.01616 val_acc= 1.00000 time= 4.45360\n",
      "Epoch: 0218 train_loss= 1051.66040 train_acc= 0.99995 val_loss= 14.71124 val_acc= 1.00000 time= 4.42234\n",
      "Epoch: 0219 train_loss= 1160.05798 train_acc= 0.99995 val_loss= 14.53620 val_acc= 1.00000 time= 4.42251\n",
      "Epoch: 0220 train_loss= 377.89429 train_acc= 0.99995 val_loss= 14.99568 val_acc= 1.00000 time= 4.49233\n",
      "Epoch: 0221 train_loss= 673.15869 train_acc= 0.99995 val_loss= 16.08740 val_acc= 1.00000 time= 4.47643\n",
      "Epoch: 0222 train_loss= 576.36133 train_acc= 0.99995 val_loss= 18.27438 val_acc= 1.00000 time= 4.47006\n",
      "Epoch: 0223 train_loss= 536.80292 train_acc= 0.99995 val_loss= 19.04253 val_acc= 1.00000 time= 4.42248\n",
      "Epoch: 0224 train_loss= 448.52103 train_acc= 0.99995 val_loss= 20.51728 val_acc= 1.00000 time= 4.48485\n",
      "Epoch: 0225 train_loss= 499.15189 train_acc= 0.99995 val_loss= 21.78498 val_acc= 1.00000 time= 4.43803\n",
      "Epoch: 0226 train_loss= 548.46375 train_acc= 0.99995 val_loss= 21.27444 val_acc= 1.00000 time= 4.45360\n",
      "Epoch: 0227 train_loss= 462.81973 train_acc= 0.99995 val_loss= 20.42583 val_acc= 1.00000 time= 4.43797\n",
      "Epoch: 0228 train_loss= 588.28619 train_acc= 0.99995 val_loss= 18.47760 val_acc= 1.00000 time= 4.53367\n",
      "Epoch: 0229 train_loss= 261.70190 train_acc= 0.99995 val_loss= 16.94314 val_acc= 1.00000 time= 4.50057\n",
      "Epoch: 0230 train_loss= 1260.58582 train_acc= 0.99995 val_loss= 14.80475 val_acc= 1.00000 time= 4.39109\n",
      "Epoch: 0231 train_loss= 781.17535 train_acc= 0.99995 val_loss= 12.85269 val_acc= 1.00000 time= 4.37117\n",
      "Epoch: 0232 train_loss= 399.67197 train_acc= 0.99995 val_loss= 10.66418 val_acc= 1.00000 time= 4.41374\n",
      "Epoch: 0233 train_loss= 479.74786 train_acc= 0.99995 val_loss= 7.57066 val_acc= 1.00000 time= 5.37220\n",
      "Epoch: 0234 train_loss= 388.07898 train_acc= 0.99995 val_loss= 4.79005 val_acc= 1.00000 time= 4.70368\n",
      "Epoch: 0235 train_loss= 636.83514 train_acc= 0.99995 val_loss= 3.05426 val_acc= 1.00000 time= 4.54735\n",
      "Epoch: 0236 train_loss= 873.84930 train_acc= 0.99995 val_loss= 1.17271 val_acc= 1.00000 time= 4.74007\n",
      "Epoch: 0237 train_loss= 601.73212 train_acc= 0.99995 val_loss= 0.22367 val_acc= 1.00000 time= 4.48557\n",
      "Epoch: 0238 train_loss= 240.02583 train_acc= 0.99995 val_loss= 0.01169 val_acc= 1.00000 time= 4.59454\n",
      "Epoch: 0239 train_loss= 359.37637 train_acc= 0.99995 val_loss= 0.53988 val_acc= 1.00000 time= 4.37351\n",
      "Epoch: 0240 train_loss= 245.88402 train_acc= 0.99995 val_loss= 1.56346 val_acc= 1.00000 time= 4.34435\n",
      "Epoch: 0241 train_loss= 277.90735 train_acc= 0.99995 val_loss= 2.48066 val_acc= 1.00000 time= 4.37768\n",
      "Epoch: 0242 train_loss= 261.99539 train_acc= 0.99995 val_loss= 3.00388 val_acc= 1.00000 time= 4.39109\n",
      "Epoch: 0243 train_loss= 246.38071 train_acc= 0.99995 val_loss= 3.52066 val_acc= 1.00000 time= 4.41865\n",
      "Epoch: 0244 train_loss= 346.65393 train_acc= 0.99995 val_loss= 3.46846 val_acc= 1.00000 time= 4.40682\n",
      "Epoch: 0245 train_loss= 219.79109 train_acc= 0.99995 val_loss= 3.33754 val_acc= 1.00000 time= 4.42234\n",
      "Epoch: 0246 train_loss= 403.22592 train_acc= 0.99995 val_loss= 2.51993 val_acc= 1.00000 time= 4.39140\n",
      "Epoch: 0247 train_loss= 369.87970 train_acc= 0.99995 val_loss= 1.49039 val_acc= 1.00000 time= 4.37568\n",
      "Epoch: 0248 train_loss= 190.61201 train_acc= 0.99995 val_loss= 0.81731 val_acc= 1.00000 time= 4.37130\n",
      "Epoch: 0249 train_loss= 405.10666 train_acc= 0.99995 val_loss= 0.35782 val_acc= 1.00000 time= 4.37570\n",
      "Epoch: 0250 train_loss= 252.07980 train_acc= 0.99995 val_loss= 0.16933 val_acc= 1.00000 time= 4.35983\n",
      "Epoch: 0251 train_loss= 200.85359 train_acc= 0.99995 val_loss= 0.04033 val_acc= 1.00000 time= 4.35992\n",
      "Epoch: 0252 train_loss= 355.76169 train_acc= 0.99995 val_loss= 0.00627 val_acc= 1.00000 time= 4.34421\n",
      "Epoch: 0253 train_loss= 1023.64777 train_acc= 0.99995 val_loss= 0.05269 val_acc= 1.00000 time= 4.56683\n",
      "Epoch: 0254 train_loss= 328.99878 train_acc= 0.99995 val_loss= 0.01643 val_acc= 1.00000 time= 4.37574\n",
      "Epoch: 0255 train_loss= 214.82951 train_acc= 0.99995 val_loss= 0.00482 val_acc= 1.00000 time= 4.39129\n",
      "Epoch: 0256 train_loss= 582.80286 train_acc= 0.99995 val_loss= 0.02793 val_acc= 1.00000 time= 4.37583\n",
      "Epoch: 0257 train_loss= 342.62256 train_acc= 0.99995 val_loss= 0.22696 val_acc= 1.00000 time= 4.37553\n",
      "Epoch: 0258 train_loss= 177.85802 train_acc= 0.99995 val_loss= 0.60640 val_acc= 1.00000 time= 4.39124\n",
      "Epoch: 0259 train_loss= 242.47757 train_acc= 0.99995 val_loss= 1.05093 val_acc= 1.00000 time= 4.35997\n",
      "Epoch: 0260 train_loss= 220.22141 train_acc= 0.99995 val_loss= 1.45621 val_acc= 1.00000 time= 4.37546\n",
      "Epoch: 0261 train_loss= 374.64001 train_acc= 0.99995 val_loss= 2.37126 val_acc= 1.00000 time= 4.43803\n",
      "Epoch: 0262 train_loss= 401.94104 train_acc= 0.99995 val_loss= 3.70842 val_acc= 1.00000 time= 4.35984\n",
      "Epoch: 0263 train_loss= 307.00436 train_acc= 0.99995 val_loss= 5.86131 val_acc= 1.00000 time= 4.35747\n",
      "Epoch: 0264 train_loss= 370.76605 train_acc= 0.99995 val_loss= 8.43023 val_acc= 1.00000 time= 4.40675\n",
      "Early stopping...\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "train_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "    \n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    validation_loss.append(cost)\n",
    "    validation_accuracy.append(acc)\n",
    "    train_loss.append(outs[1])\n",
    "    train_accuracy.append(outs[2])\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and validation_loss[-1] > np.mean(validation_loss[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
