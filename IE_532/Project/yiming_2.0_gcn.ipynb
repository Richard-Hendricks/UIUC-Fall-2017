{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#Reference\" data-toc-modified-id=\"Reference-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Reference</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#Code\" data-toc-modified-id=\"Code-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Code</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#generate-graph\" data-toc-modified-id=\"generate-graph-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>generate graph</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#simple-neural-network\" data-toc-modified-id=\"simple-neural-network-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>simple neural network</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/17F532IE/PageRank/Draft.ipynb#测试前要看一下\" data-toc-modified-id=\"测试前要看一下-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>测试前要看一下</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@bgoncalves/network-effects-explained-pagerank-and-preferential-attachment-61fdf93d023a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"web-Stanford.txt\",\"r\") as fil:\n",
    "    for _ in range(4):\n",
    "        next(fil)\n",
    "    data=[tuple([int(j) for j in i.strip().split()]) for i in fil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from(data)\n",
    "\n",
    "pr = nx.pagerank(G, alpha=0.9)   #作为true response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in pr.keys():\n",
    "    G[i]['score']=pr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.561215144991757e-07"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G[1]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=len(G.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferential Attachment Model\n",
    "命名和上一个一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_temp = nx.barabasi_albert_graph(1000, 1)\n",
    "\n",
    "# initialize an empty directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(G_temp.edges())\n",
    "\n",
    "pr = nx.pagerank(G, alpha = 0.85) \n",
    "for i in pr.keys():\n",
    "    G[i]['score']=pr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007987986538502552"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G[1]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=len(G.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gcn.utils import *\n",
    "from gcn.models import Model,MLP\n",
    "from gcn.layers import *\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试前要看一下\n",
    "+ 之前学习率填了0.05，明显训练的时候有了误差来回变大减小又变大再减小的情况，应该要调小一点，之前作业里是0.01,我一时兴起又改大了……\n",
    "+ 用networkx里的pagerank算法出来的分数是在0-1之间，所以最后的误差出来即使是0.几，也不确定是不是很好，这里是100个点做validation。\n",
    "+ epoch可能也要调，学习率0.05的时候300差不多收敛了，但是不知道学习率改了之后是什么情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'  #这行应该并没有什么用\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.05, 'Initial learning rate.')\n",
    "#flags.DEFINE_float('learning_rate', 0.1, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 300, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "#flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "#flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 100, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "    # Measures the probability error in discrete classification tasks in which the classes are mutually exclusive \n",
    "    # 这里还可以试别的loss function\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels) \n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "#--------------- Your code here --------------#\n",
    "\n",
    "def masked_mean_square_error(preds,labels,mask):\n",
    "    \"\"\"L2 loss with masking.\"\"\"\n",
    "    loss = tf.nn.l2_loss(preds - labels)   \n",
    "    # loss = tf.nn.relu(preds - labels) Rulu loss\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "#--------------------------------------------#\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_mean_square_error(self.outputs, self.placeholders['labels'],\n",
    "                                                 self.placeholders['labels_mask'])\n",
    "#        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "#                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=FLAGS.hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return self.outputs    #tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "# A^2\n",
    "#    adj_square = np.power(adj,2).tocoo()\n",
    "#    return sparse_to_tuple(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly generate graph with 500 nodes\n",
    "adj = nx.adjacency_matrix(G)\n",
    "\n",
    "# Generate feature matrix  #there is difference between different version of networkx, 这里是用networkx 2.0. 原来1.1的用法注释掉了\n",
    "#features = sp.vstack(list(j[1] for j in G.in_degree(G.nodes())),dtype = 'float32').tolil()\n",
    "features = sp.vstack(list(G.in_degree(G.nodes()).values()),dtype = 'float32').tolil()   \n",
    "\n",
    "# Feed labels to 20 nodes\n",
    "label = []\n",
    "for i in G.nodes():\n",
    "    #label.append(G.nodes[i]['score']) 我用的1.1语法-YG\n",
    "    label.append(G[i]['score'])\n",
    "        \n",
    "alabel = np.array(label, dtype='float64')\n",
    "\n",
    "idx_train = range(500)\n",
    "idx_val = range(500, 500 + 100) #20000改到500 for preferential attachment model\n",
    "\n",
    "y_train = np.zeros([n, 1])   #n is number of total nodes, =281903\n",
    "train_mask = np.zeros([n, ], dtype = 'bool')\n",
    "for i in idx_train:\n",
    "    y_train[i] = alabel[i]\n",
    "    train_mask[i] = True\n",
    "\n",
    "y_val = np.zeros([n, 1])\n",
    "val_mask = np.zeros([n, ], dtype = 'bool')\n",
    "for i in idx_val:\n",
    "    y_val[i] = alabel[i]\n",
    "    val_mask[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yiming/Desktop/Homework/IE 532/Project/gcn/utils.py:97: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "#adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 246.09235 train_acc= 1.00000 val_loss= 0.04680 val_acc= 1.00000 time= 0.02910\n",
      "Epoch: 0002 train_loss= 156.10789 train_acc= 1.00000 val_loss= 0.34567 val_acc= 1.00000 time= 0.00303\n",
      "Epoch: 0003 train_loss= 93.22315 train_acc= 1.00000 val_loss= 0.35612 val_acc= 1.00000 time= 0.00308\n",
      "Epoch: 0004 train_loss= 53.68621 train_acc= 1.00000 val_loss= 0.27515 val_acc= 1.00000 time= 0.00291\n",
      "Epoch: 0005 train_loss= 37.78463 train_acc= 1.00000 val_loss= 0.03137 val_acc= 1.00000 time= 0.00304\n",
      "Epoch: 0006 train_loss= 20.94411 train_acc= 1.00000 val_loss= 0.08031 val_acc= 1.00000 time= 0.00296\n",
      "Epoch: 0007 train_loss= 8.98665 train_acc= 1.00000 val_loss= 0.10950 val_acc= 1.00000 time= 0.00284\n",
      "Epoch: 0008 train_loss= 4.88086 train_acc= 1.00000 val_loss= 0.05397 val_acc= 1.00000 time= 0.00287\n",
      "Epoch: 0009 train_loss= 2.51449 train_acc= 1.00000 val_loss= 0.00189 val_acc= 1.00000 time= 0.02035\n",
      "Epoch: 0010 train_loss= 1.45273 train_acc= 1.00000 val_loss= 0.01212 val_acc= 1.00000 time= 0.00494\n",
      "Epoch: 0011 train_loss= 1.00226 train_acc= 1.00000 val_loss= 0.30022 val_acc= 1.00000 time= 0.00504\n",
      "Epoch: 0012 train_loss= 1.08945 train_acc= 1.00000 val_loss= 0.21887 val_acc= 1.00000 time= 0.00379\n",
      "Epoch: 0013 train_loss= 0.70298 train_acc= 1.00000 val_loss= 0.09168 val_acc= 1.00000 time= 0.00491\n",
      "Epoch: 0014 train_loss= 0.63771 train_acc= 1.00000 val_loss= 0.01522 val_acc= 1.00000 time= 0.00411\n",
      "Epoch: 0015 train_loss= 0.52393 train_acc= 1.00000 val_loss= 0.01979 val_acc= 1.00000 time= 0.00316\n",
      "Epoch: 0016 train_loss= 0.47988 train_acc= 1.00000 val_loss= 0.04883 val_acc= 1.00000 time= 0.00496\n",
      "Epoch: 0017 train_loss= 0.39692 train_acc= 1.00000 val_loss= 0.07426 val_acc= 1.00000 time= 0.00343\n",
      "Epoch: 0018 train_loss= 0.33607 train_acc= 1.00000 val_loss= 0.08049 val_acc= 1.00000 time= 0.00295\n",
      "Epoch: 0019 train_loss= 0.29968 train_acc= 1.00000 val_loss= 0.06427 val_acc= 1.00000 time= 0.00275\n",
      "Epoch: 0020 train_loss= 0.23773 train_acc= 1.00000 val_loss= 0.03903 val_acc= 1.00000 time= 0.00318\n",
      "Epoch: 0021 train_loss= 0.16717 train_acc= 1.00000 val_loss= 0.01736 val_acc= 1.00000 time= 0.00344\n",
      "Epoch: 0022 train_loss= 0.12180 train_acc= 1.00000 val_loss= 0.00580 val_acc= 1.00000 time= 0.00301\n",
      "Epoch: 0023 train_loss= 0.09511 train_acc= 1.00000 val_loss= 0.00193 val_acc= 1.00000 time= 0.00296\n",
      "Epoch: 0024 train_loss= 0.07003 train_acc= 1.00000 val_loss= 0.00140 val_acc= 1.00000 time= 0.00295\n",
      "Epoch: 0025 train_loss= 0.05330 train_acc= 1.00000 val_loss= 0.00175 val_acc= 1.00000 time= 0.00302\n",
      "Epoch: 0026 train_loss= 0.03154 train_acc= 1.00000 val_loss= 0.00231 val_acc= 1.00000 time= 0.00307\n",
      "Epoch: 0027 train_loss= 0.02077 train_acc= 1.00000 val_loss= 0.00252 val_acc= 1.00000 time= 0.00302\n",
      "Epoch: 0028 train_loss= 0.01103 train_acc= 1.00000 val_loss= 0.00210 val_acc= 1.00000 time= 0.00311\n",
      "Epoch: 0029 train_loss= 0.00617 train_acc= 1.00000 val_loss= 0.00132 val_acc= 1.00000 time= 0.00359\n",
      "Epoch: 0030 train_loss= 0.00338 train_acc= 1.00000 val_loss= 0.00060 val_acc= 1.00000 time= 0.00381\n",
      "Epoch: 0031 train_loss= 0.00107 train_acc= 1.00000 val_loss= 0.00022 val_acc= 1.00000 time= 0.00458\n",
      "Epoch: 0032 train_loss= 0.00034 train_acc= 1.00000 val_loss= 0.00022 val_acc= 1.00000 time= 0.00317\n",
      "Epoch: 0033 train_loss= 0.00045 train_acc= 1.00000 val_loss= 0.00030 val_acc= 1.00000 time= 0.00285\n",
      "Epoch: 0034 train_loss= 0.00066 train_acc= 1.00000 val_loss= 0.00045 val_acc= 1.00000 time= 0.00289\n",
      "Epoch: 0035 train_loss= 0.00098 train_acc= 1.00000 val_loss= 0.00060 val_acc= 1.00000 time= 0.00306\n",
      "Epoch: 0036 train_loss= 0.00131 train_acc= 1.00000 val_loss= 0.00069 val_acc= 1.00000 time= 0.00310\n",
      "Epoch: 0037 train_loss= 0.00164 train_acc= 1.00000 val_loss= 0.00077 val_acc= 1.00000 time= 0.00303\n",
      "Epoch: 0038 train_loss= 0.00191 train_acc= 1.00000 val_loss= 0.00083 val_acc= 1.00000 time= 0.00312\n",
      "Epoch: 0039 train_loss= 0.00200 train_acc= 1.00000 val_loss= 0.00087 val_acc= 1.00000 time= 0.00304\n",
      "Epoch: 0040 train_loss= 0.00181 train_acc= 1.00000 val_loss= 0.00090 val_acc= 1.00000 time= 0.00302\n",
      "Epoch: 0041 train_loss= 0.00228 train_acc= 1.00000 val_loss= 0.00092 val_acc= 1.00000 time= 0.00299\n",
      "Epoch: 0042 train_loss= 0.00231 train_acc= 1.00000 val_loss= 0.00093 val_acc= 1.00000 time= 0.00352\n",
      "Epoch: 0043 train_loss= 0.00215 train_acc= 1.00000 val_loss= 0.00093 val_acc= 1.00000 time= 0.00391\n",
      "Epoch: 0044 train_loss= 0.00223 train_acc= 1.00000 val_loss= 0.00093 val_acc= 1.00000 time= 0.00353\n",
      "Epoch: 0045 train_loss= 0.00229 train_acc= 1.00000 val_loss= 0.00092 val_acc= 1.00000 time= 0.00361\n",
      "Epoch: 0046 train_loss= 0.00231 train_acc= 1.00000 val_loss= 0.00090 val_acc= 1.00000 time= 0.00407\n",
      "Epoch: 0047 train_loss= 0.00203 train_acc= 1.00000 val_loss= 0.00088 val_acc= 1.00000 time= 0.00522\n",
      "Epoch: 0048 train_loss= 0.00228 train_acc= 1.00000 val_loss= 0.00086 val_acc= 1.00000 time= 0.00443\n",
      "Epoch: 0049 train_loss= 0.00207 train_acc= 1.00000 val_loss= 0.00084 val_acc= 1.00000 time= 0.00789\n",
      "Epoch: 0050 train_loss= 0.00217 train_acc= 1.00000 val_loss= 0.00082 val_acc= 1.00000 time= 0.00387\n",
      "Epoch: 0051 train_loss= 0.00192 train_acc= 1.00000 val_loss= 0.00080 val_acc= 1.00000 time= 0.00357\n",
      "Epoch: 0052 train_loss= 0.00181 train_acc= 1.00000 val_loss= 0.00077 val_acc= 1.00000 time= 0.00629\n",
      "Epoch: 0053 train_loss= 0.00188 train_acc= 1.00000 val_loss= 0.00075 val_acc= 1.00000 time= 0.00339\n",
      "Epoch: 0054 train_loss= 0.00179 train_acc= 1.00000 val_loss= 0.00073 val_acc= 1.00000 time= 0.00359\n",
      "Epoch: 0055 train_loss= 0.00171 train_acc= 1.00000 val_loss= 0.00070 val_acc= 1.00000 time= 0.00290\n",
      "Epoch: 0056 train_loss= 0.00153 train_acc= 1.00000 val_loss= 0.00068 val_acc= 1.00000 time= 0.00344\n",
      "Epoch: 0057 train_loss= 0.00166 train_acc= 1.00000 val_loss= 0.00066 val_acc= 1.00000 time= 0.00289\n",
      "Epoch: 0058 train_loss= 0.00150 train_acc= 1.00000 val_loss= 0.00064 val_acc= 1.00000 time= 0.00329\n",
      "Epoch: 0059 train_loss= 0.00144 train_acc= 1.00000 val_loss= 0.00062 val_acc= 1.00000 time= 0.00320\n",
      "Epoch: 0060 train_loss= 0.00132 train_acc= 1.00000 val_loss= 0.00060 val_acc= 1.00000 time= 0.00345\n",
      "Epoch: 0061 train_loss= 0.00133 train_acc= 1.00000 val_loss= 0.00059 val_acc= 1.00000 time= 0.00310\n",
      "Epoch: 0062 train_loss= 0.00127 train_acc= 1.00000 val_loss= 0.00057 val_acc= 1.00000 time= 0.00334\n",
      "Epoch: 0063 train_loss= 0.00131 train_acc= 1.00000 val_loss= 0.00055 val_acc= 1.00000 time= 0.00376\n",
      "Epoch: 0064 train_loss= 0.00118 train_acc= 1.00000 val_loss= 0.00054 val_acc= 1.00000 time= 0.00378\n",
      "Epoch: 0065 train_loss= 0.00115 train_acc= 1.00000 val_loss= 0.00052 val_acc= 1.00000 time= 0.00282\n",
      "Epoch: 0066 train_loss= 0.00116 train_acc= 1.00000 val_loss= 0.00051 val_acc= 1.00000 time= 0.00328\n",
      "Epoch: 0067 train_loss= 0.00112 train_acc= 1.00000 val_loss= 0.00050 val_acc= 1.00000 time= 0.00520\n",
      "Epoch: 0068 train_loss= 0.00107 train_acc= 1.00000 val_loss= 0.00048 val_acc= 1.00000 time= 0.00319\n",
      "Epoch: 0069 train_loss= 0.00101 train_acc= 1.00000 val_loss= 0.00047 val_acc= 1.00000 time= 0.00802\n",
      "Epoch: 0070 train_loss= 0.00096 train_acc= 1.00000 val_loss= 0.00046 val_acc= 1.00000 time= 0.00665\n",
      "Epoch: 0071 train_loss= 0.00093 train_acc= 1.00000 val_loss= 0.00045 val_acc= 1.00000 time= 0.00450\n",
      "Epoch: 0072 train_loss= 0.00101 train_acc= 1.00000 val_loss= 0.00044 val_acc= 1.00000 time= 0.00341\n",
      "Epoch: 0073 train_loss= 0.00091 train_acc= 1.00000 val_loss= 0.00043 val_acc= 1.00000 time= 0.00429\n",
      "Epoch: 0074 train_loss= 0.00086 train_acc= 1.00000 val_loss= 0.00042 val_acc= 1.00000 time= 0.00805\n",
      "Epoch: 0075 train_loss= 0.00089 train_acc= 1.00000 val_loss= 0.00041 val_acc= 1.00000 time= 0.00768\n",
      "Epoch: 0076 train_loss= 0.00082 train_acc= 1.00000 val_loss= 0.00041 val_acc= 1.00000 time= 0.00359\n",
      "Epoch: 0077 train_loss= 0.00081 train_acc= 1.00000 val_loss= 0.00040 val_acc= 1.00000 time= 0.00507\n",
      "Epoch: 0078 train_loss= 0.00077 train_acc= 1.00000 val_loss= 0.00039 val_acc= 1.00000 time= 0.00640\n",
      "Epoch: 0079 train_loss= 0.00076 train_acc= 1.00000 val_loss= 0.00038 val_acc= 1.00000 time= 0.00728\n",
      "Epoch: 0080 train_loss= 0.00077 train_acc= 1.00000 val_loss= 0.00038 val_acc= 1.00000 time= 0.00845\n",
      "Epoch: 0081 train_loss= 0.00071 train_acc= 1.00000 val_loss= 0.00037 val_acc= 1.00000 time= 0.00532\n",
      "Epoch: 0082 train_loss= 0.00070 train_acc= 1.00000 val_loss= 0.00037 val_acc= 1.00000 time= 0.00692\n",
      "Epoch: 0083 train_loss= 0.00071 train_acc= 1.00000 val_loss= 0.00036 val_acc= 1.00000 time= 0.01901\n",
      "Epoch: 0084 train_loss= 0.00070 train_acc= 1.00000 val_loss= 0.00036 val_acc= 1.00000 time= 0.00467\n",
      "Epoch: 0085 train_loss= 0.00065 train_acc= 1.00000 val_loss= 0.00035 val_acc= 1.00000 time= 0.00510\n",
      "Epoch: 0086 train_loss= 0.00066 train_acc= 1.00000 val_loss= 0.00035 val_acc= 1.00000 time= 0.00644\n",
      "Epoch: 0087 train_loss= 0.00065 train_acc= 1.00000 val_loss= 0.00034 val_acc= 1.00000 time= 0.00884\n",
      "Epoch: 0088 train_loss= 0.00064 train_acc= 1.00000 val_loss= 0.00034 val_acc= 1.00000 time= 0.00489\n",
      "Epoch: 0089 train_loss= 0.00062 train_acc= 1.00000 val_loss= 0.00033 val_acc= 1.00000 time= 0.00496\n",
      "Epoch: 0090 train_loss= 0.00061 train_acc= 1.00000 val_loss= 0.00033 val_acc= 1.00000 time= 0.00683\n",
      "Epoch: 0091 train_loss= 0.00057 train_acc= 1.00000 val_loss= 0.00033 val_acc= 1.00000 time= 0.00440\n",
      "Epoch: 0092 train_loss= 0.00061 train_acc= 1.00000 val_loss= 0.00032 val_acc= 1.00000 time= 0.00423\n",
      "Epoch: 0093 train_loss= 0.00058 train_acc= 1.00000 val_loss= 0.00032 val_acc= 1.00000 time= 0.00366\n",
      "Epoch: 0094 train_loss= 0.00055 train_acc= 1.00000 val_loss= 0.00032 val_acc= 1.00000 time= 0.00459\n",
      "Epoch: 0095 train_loss= 0.00057 train_acc= 1.00000 val_loss= 0.00031 val_acc= 1.00000 time= 0.00690\n",
      "Epoch: 0096 train_loss= 0.00056 train_acc= 1.00000 val_loss= 0.00031 val_acc= 1.00000 time= 0.00388\n",
      "Epoch: 0097 train_loss= 0.00055 train_acc= 1.00000 val_loss= 0.00031 val_acc= 1.00000 time= 0.00330\n",
      "Epoch: 0098 train_loss= 0.00053 train_acc= 1.00000 val_loss= 0.00030 val_acc= 1.00000 time= 0.00461\n",
      "Epoch: 0099 train_loss= 0.00053 train_acc= 1.00000 val_loss= 0.00030 val_acc= 1.00000 time= 0.00377\n",
      "Epoch: 0100 train_loss= 0.00051 train_acc= 1.00000 val_loss= 0.00030 val_acc= 1.00000 time= 0.00341\n",
      "Epoch: 0101 train_loss= 0.00051 train_acc= 1.00000 val_loss= 0.00030 val_acc= 1.00000 time= 0.00553\n",
      "Epoch: 0102 train_loss= 0.00051 train_acc= 1.00000 val_loss= 0.00030 val_acc= 1.00000 time= 0.00343\n",
      "Epoch: 0103 train_loss= 0.00050 train_acc= 1.00000 val_loss= 0.00029 val_acc= 1.00000 time= 0.00328\n",
      "Epoch: 0104 train_loss= 0.00049 train_acc= 1.00000 val_loss= 0.00029 val_acc= 1.00000 time= 0.00366\n",
      "Epoch: 0105 train_loss= 0.00048 train_acc= 1.00000 val_loss= 0.00029 val_acc= 1.00000 time= 0.00369\n",
      "Epoch: 0106 train_loss= 0.00048 train_acc= 1.00000 val_loss= 0.00029 val_acc= 1.00000 time= 0.00325\n",
      "Epoch: 0107 train_loss= 0.00049 train_acc= 1.00000 val_loss= 0.00029 val_acc= 1.00000 time= 0.00323\n",
      "Epoch: 0108 train_loss= 0.00047 train_acc= 1.00000 val_loss= 0.00028 val_acc= 1.00000 time= 0.00289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0109 train_loss= 0.00047 train_acc= 1.00000 val_loss= 0.00028 val_acc= 1.00000 time= 0.00288\n",
      "Epoch: 0110 train_loss= 0.00045 train_acc= 1.00000 val_loss= 0.00028 val_acc= 1.00000 time= 0.00465\n",
      "Epoch: 0111 train_loss= 0.00046 train_acc= 1.00000 val_loss= 0.00028 val_acc= 1.00000 time= 0.00391\n",
      "Epoch: 0112 train_loss= 0.00046 train_acc= 1.00000 val_loss= 0.00028 val_acc= 1.00000 time= 0.00362\n",
      "Epoch: 0113 train_loss= 0.00045 train_acc= 1.00000 val_loss= 0.00028 val_acc= 1.00000 time= 0.00370\n",
      "Epoch: 0114 train_loss= 0.00046 train_acc= 1.00000 val_loss= 0.00028 val_acc= 1.00000 time= 0.00455\n",
      "Epoch: 0115 train_loss= 0.00045 train_acc= 1.00000 val_loss= 0.00028 val_acc= 1.00000 time= 0.01028\n",
      "Epoch: 0116 train_loss= 0.00044 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00385\n",
      "Epoch: 0117 train_loss= 0.00044 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00647\n",
      "Epoch: 0118 train_loss= 0.00044 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00469\n",
      "Epoch: 0119 train_loss= 0.00043 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00658\n",
      "Epoch: 0120 train_loss= 0.00043 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00803\n",
      "Epoch: 0121 train_loss= 0.00044 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00383\n",
      "Epoch: 0122 train_loss= 0.00043 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00589\n",
      "Epoch: 0123 train_loss= 0.00042 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00325\n",
      "Epoch: 0124 train_loss= 0.00042 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00321\n",
      "Epoch: 0125 train_loss= 0.00042 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00314\n",
      "Epoch: 0126 train_loss= 0.00042 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00344\n",
      "Epoch: 0127 train_loss= 0.00041 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00300\n",
      "Epoch: 0128 train_loss= 0.00041 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00620\n",
      "Epoch: 0129 train_loss= 0.00041 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00346\n",
      "Epoch: 0130 train_loss= 0.00041 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00301\n",
      "Epoch: 0131 train_loss= 0.00041 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00337\n",
      "Epoch: 0132 train_loss= 0.00041 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00307\n",
      "Epoch: 0133 train_loss= 0.00041 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00322\n",
      "Epoch: 0134 train_loss= 0.00040 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00292\n",
      "Epoch: 0135 train_loss= 0.00040 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00298\n",
      "Epoch: 0136 train_loss= 0.00040 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00310\n",
      "Epoch: 0137 train_loss= 0.00040 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00306\n",
      "Epoch: 0138 train_loss= 0.00040 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00284\n",
      "Epoch: 0139 train_loss= 0.00040 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00292\n",
      "Epoch: 0140 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00304\n",
      "Epoch: 0141 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00288\n",
      "Epoch: 0142 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00297\n",
      "Epoch: 0143 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00282\n",
      "Epoch: 0144 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00311\n",
      "Epoch: 0145 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00363\n",
      "Epoch: 0146 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00392\n",
      "Epoch: 0147 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00026 val_acc= 1.00000 time= 0.00338\n",
      "Epoch: 0148 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00341\n",
      "Epoch: 0149 train_loss= 0.00039 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00372\n",
      "Epoch: 0150 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00438\n",
      "Epoch: 0151 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00593\n",
      "Epoch: 0152 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00525\n",
      "Epoch: 0153 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00322\n",
      "Epoch: 0154 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00353\n",
      "Epoch: 0155 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00335\n",
      "Epoch: 0156 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00458\n",
      "Epoch: 0157 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00317\n",
      "Epoch: 0158 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00318\n",
      "Epoch: 0159 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00298\n",
      "Epoch: 0160 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00299\n",
      "Epoch: 0161 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00311\n",
      "Epoch: 0162 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00297\n",
      "Epoch: 0163 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00300\n",
      "Epoch: 0164 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00298\n",
      "Epoch: 0165 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00291\n",
      "Epoch: 0166 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00320\n",
      "Epoch: 0167 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00313\n",
      "Epoch: 0168 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00301\n",
      "Epoch: 0169 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00302\n",
      "Epoch: 0170 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00298\n",
      "Epoch: 0171 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00317\n",
      "Epoch: 0172 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00298\n",
      "Epoch: 0173 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00282\n",
      "Epoch: 0174 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00298\n",
      "Epoch: 0175 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00293\n",
      "Epoch: 0176 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00295\n",
      "Epoch: 0177 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00319\n",
      "Epoch: 0178 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00314\n",
      "Epoch: 0179 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00300\n",
      "Epoch: 0180 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00299\n",
      "Epoch: 0181 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00296\n",
      "Epoch: 0182 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00331\n",
      "Epoch: 0183 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00301\n",
      "Epoch: 0184 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00349\n",
      "Epoch: 0185 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00290\n",
      "Epoch: 0186 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00351\n",
      "Epoch: 0187 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00292\n",
      "Epoch: 0188 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00328\n",
      "Epoch: 0189 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00300\n",
      "Epoch: 0190 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00426\n",
      "Epoch: 0191 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00365\n",
      "Epoch: 0192 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00376\n",
      "Epoch: 0193 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00402\n",
      "Epoch: 0194 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00556\n",
      "Epoch: 0195 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00755\n",
      "Epoch: 0196 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00480\n",
      "Epoch: 0197 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00472\n",
      "Epoch: 0198 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00667\n",
      "Epoch: 0199 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00477\n",
      "Epoch: 0200 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00459\n",
      "Epoch: 0201 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00455\n",
      "Epoch: 0202 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00400\n",
      "Epoch: 0203 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00721\n",
      "Epoch: 0204 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00417\n",
      "Epoch: 0205 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00599\n",
      "Epoch: 0206 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00523\n",
      "Epoch: 0207 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00636\n",
      "Epoch: 0208 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00759\n",
      "Epoch: 0209 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00844\n",
      "Epoch: 0210 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00479\n",
      "Epoch: 0211 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00747\n",
      "Epoch: 0212 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00445\n",
      "Epoch: 0213 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00490\n",
      "Epoch: 0214 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00461\n",
      "Epoch: 0215 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00525\n",
      "Epoch: 0216 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0217 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00455\n",
      "Epoch: 0218 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00900\n",
      "Epoch: 0219 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00479\n",
      "Epoch: 0220 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00370\n",
      "Epoch: 0221 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00374\n",
      "Epoch: 0222 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00614\n",
      "Epoch: 0223 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00799\n",
      "Epoch: 0224 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00704\n",
      "Epoch: 0225 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00556\n",
      "Epoch: 0226 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00562\n",
      "Epoch: 0227 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00498\n",
      "Epoch: 0228 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00388\n",
      "Epoch: 0229 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00560\n",
      "Epoch: 0230 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00339\n",
      "Epoch: 0231 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00554\n",
      "Epoch: 0232 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00480\n",
      "Epoch: 0233 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00319\n",
      "Epoch: 0234 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00489\n",
      "Epoch: 0235 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00343\n",
      "Epoch: 0236 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00288\n",
      "Epoch: 0237 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00330\n",
      "Epoch: 0238 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00559\n",
      "Epoch: 0239 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00638\n",
      "Epoch: 0240 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00354\n",
      "Epoch: 0241 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00435\n",
      "Epoch: 0242 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00382\n",
      "Epoch: 0243 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00336\n",
      "Epoch: 0244 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00315\n",
      "Epoch: 0245 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00393\n",
      "Epoch: 0246 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00326\n",
      "Epoch: 0247 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00310\n",
      "Epoch: 0248 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00296\n",
      "Epoch: 0249 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00324\n",
      "Epoch: 0250 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00354\n",
      "Epoch: 0251 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00326\n",
      "Epoch: 0252 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00346\n",
      "Epoch: 0253 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00396\n",
      "Epoch: 0254 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00391\n",
      "Epoch: 0255 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00377\n",
      "Epoch: 0256 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00351\n",
      "Epoch: 0257 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00389\n",
      "Epoch: 0258 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00523\n",
      "Epoch: 0259 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00725\n",
      "Epoch: 0260 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00507\n",
      "Epoch: 0261 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00333\n",
      "Epoch: 0262 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00350\n",
      "Epoch: 0263 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00299\n",
      "Epoch: 0264 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00484\n",
      "Epoch: 0265 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00442\n",
      "Epoch: 0266 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00324\n",
      "Epoch: 0267 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00306\n",
      "Epoch: 0268 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00412\n",
      "Epoch: 0269 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00362\n",
      "Epoch: 0270 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00329\n",
      "Epoch: 0271 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00310\n",
      "Epoch: 0272 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00365\n",
      "Epoch: 0273 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00353\n",
      "Epoch: 0274 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00314\n",
      "Epoch: 0275 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00314\n",
      "Epoch: 0276 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00321\n",
      "Epoch: 0277 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00316\n",
      "Epoch: 0278 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00286\n",
      "Epoch: 0279 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00330\n",
      "Epoch: 0280 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00308\n",
      "Epoch: 0281 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00292\n",
      "Epoch: 0282 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00286\n",
      "Epoch: 0283 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00298\n",
      "Epoch: 0284 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00297\n",
      "Epoch: 0285 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00295\n",
      "Epoch: 0286 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00282\n",
      "Epoch: 0287 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00293\n",
      "Epoch: 0288 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00288\n",
      "Epoch: 0289 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00297\n",
      "Epoch: 0290 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00281\n",
      "Epoch: 0291 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00299\n",
      "Epoch: 0292 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00299\n",
      "Epoch: 0293 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00295\n",
      "Epoch: 0294 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00309\n",
      "Epoch: 0295 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00355\n",
      "Epoch: 0296 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00382\n",
      "Epoch: 0297 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00363\n",
      "Epoch: 0298 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00449\n",
      "Epoch: 0299 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00559\n",
      "Epoch: 0300 train_loss= 0.00038 train_acc= 1.00000 val_loss= 0.00025 val_acc= 1.00000 time= 0.00312\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "train_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "    \n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    validation_loss.append(cost)\n",
    "    validation_accuracy.append(acc)\n",
    "    train_loss.append(outs[1])\n",
    "    train_accuracy.append(outs[2])\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and validation_loss[-1] > np.mean(validation_loss[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
