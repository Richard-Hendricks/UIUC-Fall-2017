# PREDICTIVE MODELING
### 2 What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?
This is known as **dataset shift**, which could lead to inaccuracy of the model.

### 3 What are some ways I can make my model more robust to outliers?
- **Use a model that's resistant to outliers.** Tree-based models are not as affected by outliers, while regression-based models are. If we're performing a statistical test, try a non-parametric test instead of a parametric one.
- **Use a more robust error metric.** Switching from mean squared error to mean absolute difference (or Huber loss) reduces the influence of outliers.

Some changes we can make to the data:

- **Winsorize your data.** Artificially cap your data at some threshold.
- **Transform your data.** If your data has a very pronounced right tail, try a log transformation.
- **Remove the outliers.**

### 4 What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?

- MSE
  - easier to compute the gradient
  - gives higher weights to large errors
  - corresponds to maximizing the likelihood of Gaussian random variables
  - ***use it when large errors are undesirable***

- MAE
  - more robust to outliers
  
### 5 
### What error metric would you use to evaluate how good a binary classifier is?
**Accuracy**
- Definition: Proportion of instances you predict correctly.
- Strengths: Very intuitive and easy to explain
- Weaknesses: Works poorly when the signal in the data is weak compared to the signal from the class imbalance. Also, you cannot express your uncertainty about a certain prediction.

**Area under the curve (AUC)**
- Definition: Given a random positive instance and a random negative instance, the probability that you can distinguish between them.
- Strengths: Works well when you want to be able to test your ability to distinguish two classes.
- Weaknesses: You may not be able to interpret your predictions as probabilities if you use AUC, since AUC only cares about ***the rankings of your prediction scores and not their actual value***. Thus you may not be able to express your uncertainty about a prediction, or even the probability that an item is successful.

**LogLoss/ Deviance**
- Strengths: Your estimates can be interpreted as probabilities.
- Weaknesses: If you have a lot of predictions that are near the boundaries, your error metric may be very sensitive to false positives or false negatives.

**F-score in NLP, Mean Average Precision, Cohen's Kappa**

### What if the classes are imbalanced?
1. AUC
Accuracy is not appropriate here, because it becomes increasingly more useless the more unbalanced your classes are.

***AUC has a probabilistic interpretation without being sensitive to class imbalance.***

The AUC is always between 0 and 1. Given a random positive instance and a random negative instance, the **AUC is the probability that you can identify who's who.**

2. F1 score: harmonic mean of precision and recall

F1= 2 * precision * recall/(precision + recall).

### What if there are more than 2 groups?
We could calculate the F-score per class and then average the results.

### 6 What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What's the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc). 分类算法的比较。

Before looking into the algorithms, we should consider:
- Number of training samples
- Dimensionality of the feature space
- Do I expect the problem to be linearly separable?
- Are features independent?
- Are features expected to linearly dependent with the target variable?
- Is overfitting expected to be a problem?
- System's requirement
- ...

Remember: **use the least complicated algorithm that can address your needs and only go for something more complicated if strictly necessary.**

**a. Logistic Regression**
- Your features are roughly linear and the problem is linearly separable
- Robust to noise, overfitting can be avoided by using l2 or l1 regularization
- Its output can be interpreted as a probability (for ranking instead of classification)
- Set a *baseline*

**b. Support Vector Machines**
- Use Hinge Loss (objective: maximize the margin)
- Use it instead of LR because **the data may not be linearly separable**. In that case, we have to use an SVM with a non linear kernel.
- For highly dimensional space (e.g., text classification)

SVM缺点：
- inefficient to train (especially when we have many training examples)

**c. Tree Ensembles**
与LR相比的优点:
  - They do not expect linear features or even features that interact linearly
  - Can handle categorical features very well
  - Can handle high dimensional spaces as well as large number of training examples

- Random Forests: can work "out of the box", easy to tune

- Gradient Boosted Trees: have more hyper-parameters to tune, and are more prone to overfitting

**d. Deep Learning**

**e. Naive Bayes**
- Generative
- Very efficient, can handle a large training set
- incremental learner

### 7 What is regularization and where might it be helpful? What is an example of using regularization in a model?
定义: Adding a "complexity term" to the error term to eliminate very complex function is called regularization.

例子：Suppose you have a classification problem. You plan to use decision trees for the task. The complexity of decision trees is determined by their depth. Now we take the class of all decision trees, and penalize each tree in proportion to its depth. So the quantity we're trying to minimize is a linear combination of the training error and the depth.

In this way, trees with small depths are ruled out because they have large training error, and trees with large depths are ruled out because they have large depth term. The optimal tree is then somewhere in the middle.

### 8 What might it be preferable to include fewer predictors over many?
**This reduces the risk of overfitting.**

Training error decreases as we increase the complexity, but the test error starts increasing after some point because our model does not generalize to different data sets anymore. Therefore simpler models that fit data well are better. 

### 9 Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?
Run GLMs, using both content and contextual features.

- Content features: # URLs & hastags
- Contextual features: # followers& followees, the age of the account

### 10 How could you collect and analyze data to use social media to predict the weather?
Collect historical data:
- Collect the historical feed of interest. This could be based on location or social network.
- Define and label weather in the past x days for each feed.
- For each feed, find the words or phrase using with text matrix. There may be some terms like "umbrella", we could also consider feature reduction here.

With the new feed:
- Extract the text and other attributes from the feed, data manipulation
- Run KNN or other predictive models

### 12 How would you design the "people you may know" feature on LinkedIn or Facebook?重要！
LinkedIn: uses many features to calculate a connection probability among two people.
- Companies position overlap
- School education overlap

### 18 How would you build a model to predict a March Madness bracket?
One approach is a linear model to predict score differential between two teams, given their historica data as well as a lot of features for each one of the teams.

### 19 What are the methods to make a predictive model more robust to outliers?
- **Use a model that's resistant to outliers.** Tree-based models are not as affected by outliers, while regression-based models are. If we're performing a statistical test, try a non-parametric test instead of a parametric one.
- **Use a more robust error metric.** Switching from mean squared error to mean absolute difference (or Huber loss) reduces the influence of outliers.

Some changes we can make to the data:

- **Winsorize your data.** Artificially cap your data at some threshold.
- **Transform your data.** If your data has a very pronounced right tail, try a log transformation.
- **Remove the outliers.**

# PROBABILITY

# STATISTICAL INFERENCE
### 1 In an A/B test, how can you check if assignment to the various buckets was truly random?
If we have two groups and several background variables on the participants, we can run a procedure such as Hotelling's T^2 to compare them. In general this should be non-significant. If we have multiple groups, MANOVA or the discriminant function should be very poor. 

### 2 What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?
This will test your **randomizer.** In running an A/A test, you would expect the conversion rate and other metrics for your users in each group to be around the same.

If you see statistically significant results much more than or less than 5% of the time, you may have the following problems:
- The two buckets are being exposed to different pages
- The buckets are not randomized properly
- Your hypothesis test is biased and you data may violate the assumptions of the test you are using (e.g., uisng a t-test when you have heavily influential outliers).

### 6 How would you run an A/B test for many variants, say 20 or more?
Let's say Yelp is testing 20 different metrics on the browsing page - conversion rate, review rate, click on ads rate. **The more metrics you are measuring, the more likely you are to get at least one false positive.**

You can change you confidence level (Bonferroni Correction) or do family-wide tests before you dive into the individual metrics. You should be careful of spurious results.
