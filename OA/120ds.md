# PREDICTIVE MODELING
### 2 What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?
This is known as **dataset shift**, which could lead to inaccuracy of the model.

### 3 What are some ways I can make my model more robust to outliers?
- **Use a model that's resistant to outliers.** Tree-based models are not as affected by outliers, while regression-based models are. If we're performing a statistical test, try a non-parametric test instead of a parametric one.
- **Use a more robust error metric.** Switching from mean squared error to mean absolute difference (or Huber loss) reduces the influence of outliers.

Some changes we can make to the data:

- **Winsorize your data.** Artificially cap your data at some threshold.
- **Transform your data.** If your data has a very pronounced right tail, try a log transformation.
- **Remove the outliers.**

### 4 What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?

- MSE
  - easier to compute the gradient
  - gives higher weights to large errors
  - corresponds to maximizing the likelihood of Gaussian random variables
  - ***use it when large errors are undesirable***

- MAE
  - more robust to outliers
  
### 5 
#### What error metric would you use to evaluate how good a binary classifier is?
**Accuracy**
- Definition: Proportion of instances you predict correctly.
- Strengths: Very intuitive and easy to explain
- Weaknesses: Works poorly when the signal in the data is weak compared to the signal from the class imbalance. Also, you cannot express your uncertainty about a certain prediction.

**Area under the curve (AUC)**
- Definition: Given a random positive instance and a random negative instance, the probability that you can distinguish between them.
- Strengths: Works well when you want to be able to test your ability to distinguish two classes.
- Weaknesses: You may not be able to interpret your predictions as probabilities if you use AUC, since AUC only cares about ***the rankings of your prediction scores and not their actual value***. Thus you may not be able to express your uncertainty about a prediction, or even the probability that an item is successful.

**LogLoss/ Deviance**
- Strengths: Your estimates can be interpreted as probabilities.
- Weaknesses: If you have a lot of predictions that are near the boundaries, your error metric may be very sensitive to false positives or false negatives.

**F-score in NLP, Mean Average Precision, Cohen's Kappa**

#### What if the classes are imbalanced?
1. AUC
Accuracy is not appropriate here, because it becomes increasingly more useless the more unbalanced your classes are.

***AUC has a probabilistic interpretation without being sensitive to class imbalance.***

The AUC is always between 0 and 1. Given a random positive instance and a random negative instance, the **AUC is the probability that you can identify who's who.**

2. F1 score: harmonic mean of precision and recall
$$F_1 = 2\cdot \frac{precision\cdot recall}{precision + recall}$$.

#### What if there are more than 2 groups?
We could calculate the F-score per class and then average the <span style="color:blue">results</span>.
