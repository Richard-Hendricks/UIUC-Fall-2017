# PREDICTIVE MODELING
### 2 What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?
This is known as **dataset shift**, which could lead to inaccuracy of the model.

### 3 What are some ways I can make my model more robust to outliers?
- **Use a model that's resistant to outliers.** Tree-based models are not as affected by outliers, while regression-based models are. If we're performing a statistical test, try a non-parametric test instead of a parametric one.
- **Use a more robust error metric.** Switching from mean squared error to mean absolute difference (or Huber loss) reduces the influence of outliers.

Some changes we can make to the data:

- **Winsorize your data.** Artificially cap your data at some threshold.
- **Transform your data.** If your data has a very pronounced right tail, try a log transformation.
- **Remove the outliers.**

### 4 What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?

- MSE
  - easier to compute the gradient
  - gives higher weights to large errors
  - corresponds to maximizing the likelihood of Gaussian random variables
  - ***use it when large errors are undesirable***

- MAE
  - more robust to outliers
  
### 5 
### What error metric would you use to evaluate how good a binary classifier is?
**Accuracy**
- Definition: Proportion of instances you predict correctly.
- Strengths: Very intuitive and easy to explain
- Weaknesses: Works poorly when the signal in the data is weak compared to the signal from the class imbalance. Also, you cannot express your uncertainty about a certain prediction.

**Area under the curve (AUC)**
- Definition: Given a random positive instance and a random negative instance, the probability that you can distinguish between them.
- Strengths: Works well when you want to be able to test your ability to distinguish two classes.
- Weaknesses: You may not be able to interpret your predictions as probabilities if you use AUC, since AUC only cares about ***the rankings of your prediction scores and not their actual value***. Thus you may not be able to express your uncertainty about a prediction, or even the probability that an item is successful.

**LogLoss/ Deviance**
- Strengths: Your estimates can be interpreted as probabilities.
- Weaknesses: If you have a lot of predictions that are near the boundaries, your error metric may be very sensitive to false positives or false negatives.

**F-score in NLP, Mean Average Precision, Cohen's Kappa**

### What if the classes are imbalanced?
1. AUC
Accuracy is not appropriate here, because it becomes increasingly more useless the more unbalanced your classes are.

***AUC has a probabilistic interpretation without being sensitive to class imbalance.***

The AUC is always between 0 and 1. Given a random positive instance and a random negative instance, the **AUC is the probability that you can identify who's who.**

2. F1 score: harmonic mean of precision and recall

F1= 2 * precision * recall/(precision + recall).

### What if there are more than 2 groups?
We could calculate the F-score per class and then average the results.

### 6 What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What's the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc). 分类算法的比较。

Before looking into the algorithms, we should consider:
- Number of training samples
- Dimensionality of the feature space
- Do I expect the problem to be linearly separable?
- Are features independent?
- Are features expected to linearly dependent with the target variable?
- Is overfitting expected to be a problem?
- System's requirement
- ...

Remember: **use the least complicated algorithm that can address your needs and only go for something more complicated if strictly necessary.**

**a. Logistic Regression**
- Your features are roughly linear and the problem is linearly separable
- Robust to noise, overfitting can be avoided by using l2 or l1 regularization
- Its output can be interpreted as a probability (for ranking instead of classification)
- Set a *baseline*

**b. Support Vector Machines**
- Use Hinge Loss (objective: maximize the margin)
- Use it instead of LR because **the data may not be linearly separable**. In that case, we have to use an SVM with a non linear kernel.
- For highly dimensional space (e.g., text classification)

SVM缺点：
- inefficient to train (especially when we have many training examples)

**c. Tree Ensembles**
与LR相比的优点:
  - They do not expect linear features or even features that interact linearly
  - Can handle categorical features very well
  - Can handle high dimensional spaces as well as large number of training examples

- Random Forests: can work "out of the box", easy to tune

- Gradient Boosted Trees: have more hyper-parameters to tune, and are more prone to overfitting

**d. Deep Learning**

**e. Naive Bayes**
- Generative
- Very efficient, can handle a large training set
- incremental learner

### 7 What is regularization and where might it be helpful? What is an example of using regularization in a model?
定义: Adding a "complexity term" to the error term to eliminate very complex function is called regularization.

例子：Suppose you have a classification problem. You plan to use decision trees for the task. The complexity of decision trees is determined by their depth. Now we take the class of all decision trees, and penalize each tree in proportion to its depth. So the quantity we're trying to minimize is a linear combination of the training error and the depth.

In this way, trees with small depths are ruled out because they have large training error, and trees with large depths are ruled out because they have large depth term. The optimal tree is then somewhere in the middle.

### 8 What might it be preferable to include fewer predictors over many?
**This reduces the risk of overfitting.**

Training error decreases as we increase the complexity, but the test error starts increasing after some point because our model does not generalize to different data sets anymore. Therefore simpler models that fit data well are better. 

### 9 Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?
Run GLMs, using both content and contextual features.

- Content features: # URLs & hastags
- Contextual features: # followers& followees, the age of the account

### 10 How could you collect and analyze data to use social media to predict the weather?
Collect historical data:
- Collect the historical feed of interest. This could be based on location or social network.
- Define and label weather in the past x days for each feed.
- For each feed, find the words or phrase using with text matrix. There may be some terms like "umbrella", we could also consider feature reduction here.

With the new feed:
- Extract the text and other attributes from the feed, data manipulation
- Run KNN or other predictive models

### 12 How would you design the "people you may know" feature on LinkedIn or Facebook?重要！
LinkedIn: uses many features to calculate a connection probability among two people.
- Companies position overlap
- School education overlap

### 18 How would you build a model to predict a March Madness bracket?
One approach is a linear model to predict score differential between two teams, given their historica data as well as a lot of features for each one of the teams.

### 19 What are the methods to make a predictive model more robust to outliers?
- **Use a model that's resistant to outliers.** Tree-based models are not as affected by outliers, while regression-based models are. If we're performing a statistical test, try a non-parametric test instead of a parametric one.
- **Use a more robust error metric.** Switching from mean squared error to mean absolute difference (or Huber loss) reduces the influence of outliers.

Some changes we can make to the data:

- **Winsorize your data.** Artificially cap your data at some threshold.
- **Transform your data.** If your data has a very pronounced right tail, try a log transformation.
- **Remove the outliers.**

# PROBABILITY

# STATISTICAL INFERENCE
### 1 In an A/B test, how can you check if assignment to the various buckets was truly random?
If we have two groups and several background variables on the participants, we can run a procedure such as Hotelling's T^2 to compare them. In general this should be non-significant. If we have multiple groups, MANOVA or the discriminant function should be very poor. 

### 2 What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?
This will test your **randomizer.** In running an A/A test, you would expect the conversion rate and other metrics for your users in each group to be around the same.

If you see statistically significant results much more than or less than 5% of the time, you may have the following problems:
- The two buckets are being exposed to different pages
- The buckets are not randomized properly
- Your hypothesis test is biased and you data may violate the assumptions of the test you are using (e.g., uisng a t-test when you have heavily influential outliers).

### 6 How would you run an A/B test for many variants, say 20 or more?
Let's say Yelp is testing 20 different metrics on the browsing page - conversion rate, review rate, click on ads rate. **The more metrics you are measuring, the more likely you are to get at least one false positive.**

You can change you confidence level (Bonferroni Correction) or do family-wide tests before you dive into the individual metrics. You should be careful of spurious results.

### Other Items
#### Significance
The probability of failing to reject the null when it is true.希望是0.

When you are running an experiment, you are trying to disprove the null hypothesis that there is no difference between the two groups.

If the statistical test returns significant, then you conclude that **the effect is unlikely to arise from random chance alone.** If you reject something with 95% confidence, then in the case that there is no true effect, then a result like ours (or more extreme than ours) will happen in less than 5% of all possible samples.

#### Power
The probability of rejecting the null when it is false, 希望是1.

#### Randomization
It balances out the confounding variables.

By assigning 50% of users to a control group and 50% of users to a treatment group, you can eliminate any possible biases that may arise in the experiment.

#### Multi-armed bandit
Strategies that allow you to experiment with multiple things and acquire new knowledge/ insights and apply the same to smaller set of tests

Example: If you want to figure out the banner image for landing page, experiment with 20, then acquire some insights and then A/B testing for top 5-6 performing images.


# DATA ANALYSIS
### 2 What is R^2? What are some other metrics that could be better than R^2 and why?
- 定义： R-squared measures the proportion of the variation in Y explained by X for a linear regression.
- 缺点：R-squared will keep increasing by adding more and more predictors, regardless of how well they are correlated to Y
- Adjusted R-squared
  - adjusts the statistic based on the number of predictors in the model, meaning it increases only if the new term improves the model more than would be expected by chance
  - a desired goodness-of-fit statistic
  
### 3 What is the curse of dimensionality?
定义： It refers to how certain learning algorithms may perform poorly in high-dimensional data.

High dimensionality makes clustering hard, because having lots of dimensions means that everything is "far away" from each other. It's hard to know what true distance means when you have so many dimensions. That's why it's often helpful to perform PCA to reduce dimensionality before clustering.

### 4 Is more data always better?
Generally speaking, more data is always preferred. But
- It also depends on the quality of your data, for example, if your data is biased, more data won't help
- It depends on your model. If your model suffers from high bias, getting more data won't improve the results beyond a point. You need to add more features, etc.

### 5 What are advantages of plotting your data before performing analysis?
It gives you a better chance of
- Doing the right analysis
- Not completely screwing up

For example, for correlation of two continuous variables, we shouldn't start calculate without looking at the plot, because correlation coefficient assumes the two are linearly correlated.

### 6 How can you make sure that you don't analyze something that ends up meaningless?
**Proper exploratory data analysis** where we are just graphing things, testing things on small sets of the data, summarizing simple statistics, and getting rough ideas of what hypotheses you might want to pursue further.

### 7 What is the role of trial and error in data analysis? What is the role of making a hypothesis before diving in?



### 8 How can you determine which features are the most important in your model?
- Remove the correlated variables prior to selecting important variables
- Linear regression based on p-values
- Forward, backward, stepwise selection
- Lasso (automated feature selection, shrinking many of them to zero)
- Information Gain
- Fraction of variation explained
- Run a Gradient Boosting Machine or Random Forest to generate plots of relative importance and information gain for each feature in the ensembles. These models are somewhat **robust to collinearity** so you will get a decent understanding of the relative importance of the features.

### 9 How do you deal with some of your predictors being missing?
Here are three ways:
- **Remove rows with missing values** - this works well if
  - the values are missing randomly
  - if you don't lose too much of the dataset
  
- **Build another predictive model to predict the missing values**
- **Use a model that can incorporate missing data** - Like a random forest, or any tree-based method

### 10 You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear reggression, one of the weights on the predictors is negative. What could be the issue?
**Multicollinearity.**

Issue: It can increase the variance of the coefficient estimates, and make them unstable and difficult to interpret, such as switching the signs.

Dectection: VIF (variance inflation factor)

Solutions: Remove highly correlated predictors, feature selection, etc.

### 11 Let's say you're given an unfeasible amount of predictors in a predictive modeling task. What are some ways to make the prediction more feasible?
- Dimension reduction
- Feature selection

### 12 How to perform feature selection? 【重要】http://scikit-learn.org/stable/modules/feature_selection.html
a. **Univariate Feature Selection** where a statistical test is applied to each feature individually. The scikit-learn has module to do this.

b. **Recursive Feature Elimination**
- First train a model with all feature and evaluate its performance on a holdout set
- Then drop say 20% weakest features (with least absolute coefficients), and retrain on the remaining features
- Iterate until you observe a sharp drop on the predictive accuracy

c. **LASSO**: Use L1 regularier which zeroing out many coefficients

d. **Elastic Net**: combines L1 penalty with l2

e. **Tree-based**: can be used to compute feature importances, which in turn can be used to discard irrelevant features


### 13 Your linear regression didn't run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?
You're fitting as many parameters as you have observations, or close to it, meaning the sample size is very small.

### 14 You run your regression on different subset of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue there?
Usually coefficients (beta) represent the level of importance of each variable. If you see a coefficient is changing from one model (subset) to another model (another subset), it means the importance of that specific variable In each dataset is different?

### 15 What is the main idea behind ensemble learning? If I had many different models that predicted the same response variable, what might I want to do to incorporate all of the models? Would you expect this to perform better than an individual model or worse?
- 定义： an ensemble is a supervised learning technique for combining multiple *weak learners/models* to produce a *strong learner*. 

- 例子： Random forset algorithms have multiple CART models. It performs better than individual CART model by
  - classifying a new object where each tree gives "votes" for that class, and random forest chooses the classification having the most votes (classification)
  - taking average of outputs of different trees (regression)
  
- 几种
  - **Bagging**: we create random samples of the training set, then build a classifier for each sample. Finally, results of those classifiers are combined using majority voting or average. It helps reduce the variance.
  - **Boosting** fits a series of models with each successive model fit to minimize the error of previous models
  - **Stacking** First, we use multiple base classifiers to predict the class. Second, a new learner is used to combine their predictions with the aim of reducing the generalization error. 
  
- 与单个比较
  - They average out biases
  - They reduce the variance
  - They're unlikely to overfit

### 16 Given that you have wifi data in your office, how would you determine which rooms and areas are underutilized and overutilized?
(1) Collect data: of wifi-accessed computers/ laptops / cell phones, number in different time of a work day in each room

(2) Calculate mean of number of accessed devices in each room, mean of online time in each room in a week or month

(3) Generally the higher mean of number and time, the room is more utilized. We can roughly define underutilized as room with mean less than average, and vice versa

(4) We still need to compare with the usage for each room, meeting room, office room or café room, compare the mean within rooms for the similar usage.

### 23 You have 5000 people that rank 10 sushis in terms of salt- iness. How would you aggregate this data to estimate the true saltiness rank in each sushi?
Some people would take the mean rank of each sushi. If I wanted something simple, I would use the median, since ranks are (strictly speaking) ordinal and not interval.

### 27 Let’s say you’re building the recommended music engine at Spotify to recommend people music based on past listening history. How would you approach this problem?
Spotify uses collaborative filtering for music recommendations, using the idea that:
- If two songs x, y get similar ratings then they are probably similar (in genres)
- If a lot of users all listen to tracks x, y, z, then those tracks are probably similar (in genres)

# PRODUCT METRICS
### What are the best metrics for measuring user engagement on service review/ content sites such as Yelp?
Besides traditional metrics such as bounce rate and time on site, additional metrics like traffic source and landing page can be useful.
- **Conversion rate:** For Yelp, this could be reserving a table at a restaurant and claiming daily-deals
- **Number of Reviews/ Number of unique reviewers:** Yelp is about user-generated content, these two measure the user's engagements.
- **Frequency of visits:** If the frequency of visits for a user is high, there may be no cause of concern. For example, a Yelp addict may visit Yelp very often, but the time he spends each time can be very short-say, less than 1 minute. However, the number of those visits make up for the short duration.
- **"Upvotes", "Follows" and other interactions between users**: Yelp allows users to vote up quality content. At the website level, number of such interactions can be considered as a good metric for user-engagement.
- **Check-ins:** Yelp allows users to "check in" at hotels/ restaurants. If users remember to check in at our website, it means they are highly engaged.
- **Social Media Sharing:** This means the content is not only good enough to read, but also interesting enough to be shared with friends.

### 可能的features
- Publisher features, such as where the ad was displayed
- Advertiser features (type of business)
- User features (browser type)
- Interaction of the user with the advertiser, such as the number of times the user visited the advertiser website.

# COMMUNICATION
### 3 How would you explain an A/B test to an engineer with no statistics background? A linear regression?
(1) **A/B testing** is the testing of different elements of a user's experience to determine which variation helps the business achieve its goal more efficiently such as increasing conversions. This can be copy on a website, button colors, different user interfaces, etc. (https://www.quora.com/What-is-A-B-testing-1)

(2) **Linear regression** is the task of fitting a straight line through a set of points.

### 4 How would you explain a confidence interval to an engineer with no statistics background? What does 95% confidence mean?
(1) Strip away from jargon, we want to know some quantity, and we use some methods to compute an interval. If we use that method a huge number of times, 95% of the intervals we generate will contain the quantity we're looking for.

(2) 95% CI means there is a 95% chance that the confidence interval you calculated contains the true population mean.

### 5 How would you explain to a group of senior executives why data is important?
Data can help companies understand their customers. It's important for companies to dig into the data so that they can have a complete view of their customer.

### 8 What's your favorite algorithm? Can you explain it to me?
